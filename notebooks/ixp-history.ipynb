{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import bz2\n",
    "import collections\n",
    "import datetime\n",
    "import glob\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing\n",
    "import operator\n",
    "import pymongo\n",
    "import pytricia\n",
    "import re\n",
    "import requests\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['savefig.pad_inches'] = 0\n",
    "plt.rcParams['savefig.format'] = 'pdf'\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('deep')\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "figsize_full = [8.0, 5.5]\n",
    "figsize_half = [8.0, 2.75]\n",
    "figsize_a4 = [8.27, 11.7]\n",
    "figsize_a3 = [11.7, 16.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/utils\")\n",
    "import tb_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://ixp_history:ixp_history@localhost:27017/ixp_history')\n",
    "db = client['ixp_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_relations = tb_common.load_as_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_ases = tb_common.load_t1_ases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_cones = tb_common.load_customer_cones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn2pfx = tb_common.load_asn2pfx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1203119793.json.bz2 2008-02-15 23:56:33\n",
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1255535998.json.bz2 2009-10-14 16:59:58\n",
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1319749479.json.bz2 2011-10-27 22:04:39\n",
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1323122587.json.bz2 2011-12-05 22:03:07\n",
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1325935385.json.bz2 2012-01-07 11:23:05\n",
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1334963765.json.bz2 2012-04-21 00:16:05\n",
      "../code/utils/tb_common/../../../data/peeringdb/json_dumps/peeringdb.1340715850.json.bz2 2012-06-26 14:04:10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-60fff9d714f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdb_ixps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mixp_member_asn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_peeringdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/DD-PostDoc-QMUL/Project - GEANT/ixp_history/code/utils/tb_common/load_data.py\u001b[0m in \u001b[0;36mload_peeringdb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'peeringdb.(?P<timestamp>\\d{10}).json.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "pdb_ixps, ixp_member_asn = tb_common.load_peeringdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdb_ixps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-069bb28dd949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m  datetime.datetime(2016, 1, 1, 0, 0)]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_ixps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdb_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_ixps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdb_ixps' is not defined"
     ]
    }
   ],
   "source": [
    "pdb_dates = [datetime.datetime(2008, 2, 15, 23, 56, 33),\n",
    " datetime.datetime(2009, 10, 14, 16, 59, 58),\n",
    " datetime.datetime(2010, 7, 29, 0, 0),\n",
    " datetime.datetime(2011, 1, 1, 0, 0),\n",
    " datetime.datetime(2012, 1, 1, 0, 0),\n",
    " datetime.datetime(2013, 1, 1, 0, 0),\n",
    " datetime.datetime(2014, 1, 1, 0, 0),\n",
    " datetime.datetime(2015, 1, 1, 0, 0),\n",
    " datetime.datetime(2016, 1, 1, 0, 0)]\n",
    "\n",
    "for date in list(pdb_ixps.keys()):\n",
    "    if date not in pdb_dates:\n",
    "        del(pdb_ixps[date])\n",
    "        del(ixp_member_asn[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autnums = tb_common.load_autnums()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_merge = [\n",
    "    dict(\n",
    "        ts=ts,\n",
    "        t1_ases=t1_ases.keys()[np.argmin([ abs(ts - ts2) for ts2 in t1_ases.keys() ])],\n",
    "        customer_cones=customer_cones.keys()[np.argmin([ abs(ts - ts2) for ts2 in customer_cones.keys() ])],\n",
    "        asn2pfx=asn2pfx.keys()[np.argmin([ abs(ts - ts2) for ts2 in asn2pfx.keys() ])],\n",
    "        ixp_member_asn=ixp_member_asn.keys()[np.argmin([ abs(ts - ts2) for ts2 in ixp_member_asn.keys() ])],\n",
    "        as_relations=as_relations.keys()[np.argmin([ abs(ts - ts2) for ts2 in as_relations.keys() ])]\n",
    "    ) for ts in [ pd.Timestamp(np.datetime64('2006-01') + n*np.timedelta64(1, 'Y')).to_pydatetime() for n in range(11) ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame(ts_merge)\n",
    "print df_ts.to_latex()\n",
    "display(df_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caida AS rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s = requests.Session()\n",
    "page = 1\n",
    "as_rank = []\n",
    "while(True):\n",
    "    r = s.get('http://as-rank.caida.org/api/v1/asns?page=%s' % (page))\n",
    "    res_json = json.loads(r.text)\n",
    "    \n",
    "    if not res_json['data']:\n",
    "        break\n",
    "        \n",
    "    as_rank.extend(res_json['data'])\n",
    "    page += 1\n",
    "\n",
    "as_rank = map(int, as_rank)\n",
    "as_rank_dict = {v: k+1 for k, v in enumerate(as_rank)}\n",
    "display(len(as_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caida AS relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caida_as_class = {}\n",
    "\n",
    "with open(ROOT_DIR + \"data/caida/as_classification/20150801.as2types.txt\") as f_in:\n",
    "    for line in f_in:\n",
    "        line = line.strip()\n",
    "        asn, _, as_class = line.split('|')\n",
    "        caida_as_class[int(asn)] = as_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_addresses(pfx_len):\n",
    "    return 2 ** (32 - pfx_len) - 2\n",
    "\n",
    "def get_number_IPs_from_prefixes(pfx_list):\n",
    "    pyt = pytricia.PyTricia(32)\n",
    "    \n",
    "    for pfx in pfx_list:\n",
    "        pyt[pfx] = None\n",
    "        \n",
    "    return get_number_IPs_from_pyt(pyt)\n",
    "\n",
    "def get_number_IPs_from_pyt(pyt):\n",
    "    \n",
    "    number_IPs = 0\n",
    "    \n",
    "    for pfx in pyt.keys():\n",
    "        # If a prefix is covered (i.e. has a parent), only consider the covering\n",
    "        # prefix to correctly compute the number of reachable IPs.\n",
    "        # Considering all uncovered prefixes should be enough to calculate reachability.\n",
    "        if pyt.parent(pfx):\n",
    "            continue\n",
    "        else:\n",
    "            _, pfxlen = pfx.split('/')\n",
    "            number_IPs += calculate_number_of_addresses(int(pfxlen))\n",
    "    \n",
    "    return number_IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {x: 2**(32-x)-2 for x in range(0, 33)}\n",
    "def calculate_number_of_addresses(pfx_len):\n",
    "    return d[pfx_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimise_pyt(pyt):\n",
    "    pyt_new = pytricia.PyTricia(32)\n",
    "    \n",
    "    for pfx in pyt.keys():\n",
    "        if not pyt.parent(pfx):\n",
    "            pyt_new[pfx] = None\n",
    "            \n",
    "    return pyt_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimise_pfx_list(pfx_list):\n",
    "    pyt = pytricia.PyTricia(32)\n",
    "    \n",
    "    for pfx in pfx_list:\n",
    "        pyt[pfx] = None\n",
    "        \n",
    "    pyt = minimise_pyt(pyt)\n",
    "    \n",
    "    return set(pyt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of IXPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.Series({ ts: len(ixps) for ts, ixps in pdb_ixps.iteritems()})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sr.plot(kind='line', ax=ax)\n",
    "\n",
    "ax.set_ylabel('# IXPs in PeeringDB')\n",
    "ax.set_xlabel('Year')\n",
    "\n",
    "plt.savefig('../figures/ixps-per-year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [ {'ts': ts, 'region_continent': ixp_data['region_continent']} for ts, ixps in pdb_ixps.iteritems() for ixp_data in ixps.itervalues() ]\n",
    "df = pd.DataFrame(tmp)\n",
    "\n",
    "with sns.color_palette(\"hls\", 8):\n",
    "    fig, ax = plt.subplots()\n",
    "    gb = df.groupby(['ts'])['region_continent'].value_counts().unstack()\n",
    "    gb.plot(kind='bar', stacked=True, ax=ax, legend=False)\n",
    "    ax.legend(ncol=2)\n",
    "    \n",
    "    ax.set_ylabel('# IXPs in PeeringDB')\n",
    "    ax.set_xlabel('Year')\n",
    "    \n",
    "    plt.savefig('../figures/ixps-per-year-per-region')\n",
    "\n",
    "gb = gb.fillna(0)\n",
    "gb.index = [ v.strftime('%Y') for v in gb.index ]\n",
    "gb.to_csv('../csvs/ixps-per-year-per-region.csv', index_label='ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {'date': ts, 'ixp': ixp, 'member_asn': asn}\n",
    "    for ts, ixps in ixp_member_asn.iteritems()\n",
    "    for ixp, members in ixps.iteritems()\n",
    "    for asn in members\n",
    "])\n",
    "\n",
    "df2 = pd.DataFrame([\n",
    "    {'date': ts, 'ixp': ixp, 'continent': props['region_continent']}\n",
    "    for ts, ixps in pdb_ixps.iteritems()\n",
    "    for ixp, props in ixps.iteritems()\n",
    "])\n",
    "\n",
    "df = df.merge(df2)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'continent']).agg({'member_asn': 'count'}).unstack()\n",
    "gb = gb.fillna(0)\n",
    "gb.index = [ v.strftime('%Y') for v in gb.index ]\n",
    "gb.columns = gb.columns.droplevel()\n",
    "display(gb.head())\n",
    "\n",
    "gb.to_csv('../csvs/ixp-asn-per-year-per-region.csv', index_label='ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'continent', 'ixp']).agg({'member_asn': 'count'}).reset_index()\n",
    "gb = gb.groupby(['date', 'continent']).apply(lambda g: g.nlargest(5, 'member_asn'))\n",
    "#gb = gb.fillna(0)\n",
    "#gb.index = [ v.strftime('%Y') for v in gb.index ]\n",
    "#gb.columns = gb.columns.droplevel()\n",
    "display(gb.head())\n",
    "\n",
    "gb.to_excel('../xlsx/most-members-ixps-per-continent.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.iloc[-1] / gb.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'continent']).agg({'member_asn': 'count', 'ixp': 'nunique'})\n",
    "gb['avg_ixp_size'] = gb['member_asn'] / gb['ixp']\n",
    "gb = gb.unstack('continent')\n",
    "display(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "gb['avg_ixp_size'].plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'ixp', 'continent']).agg({'member_asn': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gb.reset_index().groupby(['date', 'continent']).agg({'member_asn': 'max'}).unstack('continent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Europe'] / df.drop('Europe', axis='columns').max(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1 reachability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_T1_reachability(ts):\n",
    "    \n",
    "    t1_prefixes = {\n",
    "        t1: minimise_pfx_list(\n",
    "            set(itertools.chain(*[asn2pfx[ts['asn2pfx']].get(asn, []) for asn in customer_cones[ts['customer_cones']][t1]]))\n",
    "        )\n",
    "        for t1 in t1_ases[ts['t1_ases']] \n",
    "    }\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for _ in range(len(t1_prefixes)):\n",
    "        reach = { t1: get_number_IPs_from_prefixes(pfxes) for t1, pfxes in t1_prefixes.iteritems() }\n",
    "        max_t1, max_t1_reach = max(reach.iteritems(), key=operator.itemgetter(1))\n",
    "        \n",
    "        max_t1_prefixes = t1_prefixes[max_t1]\n",
    "        \n",
    "        t1_prefixes = { t1: pfxes.union(max_t1_prefixes) for t1, pfxes in t1_prefixes.iteritems() }\n",
    "        del(t1_prefixes[max_t1])\n",
    "        \n",
    "        res.append((max_t1, max_t1_reach))\n",
    "        \n",
    "    return (ts['ts'], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "res = dict(pool.map(calc_T1_reachability, ts_merge))\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict([(k, pd.Series([x[1] for x in v])) for k, v in res.items()]))\n",
    "df = df.transpose()\n",
    "\n",
    "to_export = df.copy()\n",
    "to_export.index = [ v.strftime('%Y') for v in to_export.index ]\n",
    "to_export.to_csv('../csvs/t1-cumulative-reach.csv', index_label='ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', len(res)):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for (ts, values) in sorted(res.iteritems(), key=operator.itemgetter(0)):\n",
    "        ax.plot(zip(*values)[1], label=ts.strftime('%Y'))\n",
    "\n",
    "    ax.legend(ncol=3)\n",
    "    ax.xaxis.set_ticks([0, 5, 10, 15])\n",
    "    \n",
    "    ax.set_ylabel('# of reachable IPs')\n",
    "    ax.set_xlabel('# of T1s')\n",
    "    \n",
    "plt.savefig('../figures/t1-cumulative-reachability-per-year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all_reach = {\n",
    "    't1_reach': {ts['ts']: 0 for ts in ts_merge},\n",
    "    't1_cc_reach': {ts['ts']: 0 for ts in ts_merge},\n",
    "    'all_reach': {ts['ts']: 0 for ts in ts_merge}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in ts_merge:\n",
    "    prefixes = []\n",
    "    for t1 in t1_ases[ts['t1_ases']]:\n",
    "        prefixes += asn2pfx[ts['asn2pfx']][t1]\n",
    "    \n",
    "    res_all_reach['t1_reach'][ts['ts']] = get_number_IPs_from_prefixes(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in ts_merge:\n",
    "    prefixes = []\n",
    "    for t1 in t1_ases[ts['t1_ases']]:\n",
    "        prefixes += list(itertools.chain(*[asn2pfx[ts['asn2pfx']].get(asn, []) for asn in customer_cones[ts['customer_cones']][t1]]))\n",
    "    \n",
    "    res_all_reach['t1_cc_reach'][ts['ts']] = get_number_IPs_from_prefixes(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in ts_merge:\n",
    "    asns = list(itertools.chain(*customer_cones[ts['customer_cones']].values()))\n",
    "    prefixes = list(itertools.chain(*[asn2pfx[ts['asn2pfx']].get(asn, []) for asn in asns]))\n",
    "    res_all_reach['all_reach'][ts['ts']] = get_number_IPs_from_prefixes(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res_all_reach)\n",
    "display(df.head())\n",
    "\n",
    "to_export = df.copy()\n",
    "to_export.index = [ v.strftime('%Y') for v in to_export.index ]\n",
    "to_export.to_csv('../csvs/reach-per-year-per-class.csv', index_label='ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t1_reach'] / df['all_reach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t1_cc_reach'] / df['all_reach']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IXP reachability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_IXP_reach(pyt, ixp_pfx, number_ips_reached):\n",
    "    \n",
    "    if len(ixp_pfx) == 0:\n",
    "        return 0\n",
    "    \n",
    "    new_pfx = [pfx for pfx in ixp_pfx if pfx not in pyt]\n",
    "    \n",
    "    for pfx in new_pfx:\n",
    "        pyt[pfx] = None\n",
    "        \n",
    "    new_reach = get_number_IPs_from_pyt(pyt)\n",
    "        \n",
    "    for pfx in new_pfx:\n",
    "        del(pyt[pfx])\n",
    "        \n",
    "    return new_reach - number_ips_reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pfxes(pyt, pfx_list):\n",
    "    return set([pfx for pfx in pfx_list if not pyt.get_key(pfx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IXP reachability single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_IXP_reachability_single(ts):\n",
    "    # ts, tqdm_pos = params\n",
    "    \n",
    "    ixp_customer_cone_asn = {}\n",
    "\n",
    "    for ixp, member_asn in ixp_member_asn[ts['ixp_member_asn']].iteritems():\n",
    "        reachable_asn = []\n",
    "        for asn in member_asn:\n",
    "            if asn in t1_ases[ts['t1_ases']]:\n",
    "                continue\n",
    "            else:\n",
    "                reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "        ixp_customer_cone_asn[ixp] = set(reachable_asn)\n",
    "        \n",
    "    ixp_reachability = { ixp: get_number_IPs_from_prefixes(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in ixp_asn ]))) for ixp, ixp_asn in ixp_customer_cone_asn.iteritems() }\n",
    "    \n",
    "    return (ts['ts'], ixp_reachability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "res_ixp_single = dict(pool.map(calc_IXP_reachability_single, ts_merge))\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ixp_single_reach = pd.DataFrame(({'ts': date, 'ixp': ixp, 'reach': reach} for date, values in res_ixp_single.iteritems() for ixp, reach in values.iteritems()))\n",
    "display(df_ixp_single_reach.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for _, row in df_ixp_single_reach.iterrows():\n",
    "    ts = pdb_time_mapping[row['ts']]\n",
    "    ixp_continent = pdb_ixps[ts][row['ixp']]['region_continent']\n",
    "    s.append(ixp_continent)\n",
    "    \n",
    "s = pd.Series(s, name='continent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ixp_single_reach['continent'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ixp_single_reach.groupby(['ts', 'continent']).apply(lambda g: g.nlargest(5, 'reach')).to_excel('../xlsx/biggest-ixps-per-continent.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IXP reachbility with T1 ASes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_IXP_reachability(ts):\n",
    "    # ts, tqdm_pos = params\n",
    "    \n",
    "    ixp_customer_cone_asn = {}\n",
    "\n",
    "    for ixp, member_asn in ixp_member_asn[ts['ixp_member_asn']].iteritems():\n",
    "        reachable_asn = []\n",
    "        for asn in member_asn:\n",
    "            # if asn in t1_ases[ts['t1_ases']]:\n",
    "            #    continue\n",
    "            # else:\n",
    "            #   reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "        ixp_customer_cone_asn[ixp] = set(reachable_asn)\n",
    "        \n",
    "    ixp_prefixes = { ixp: minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in ixp_asn ]))) for ixp, ixp_asn in ixp_customer_cone_asn.iteritems() }\n",
    "\n",
    "    number_ips_reached = 0\n",
    "    pyt = pytricia.PyTricia(32)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for _ in range(len(ixp_prefixes)):\n",
    "    #for _ in tqdm.tnrange(len(ixp_prefixes), desc=str(ts['ts'].year), position=tqdm_pos):\n",
    "        tmp = [ (ixp, get_number_IPs_from_prefixes(pfx_list)) for ixp, pfx_list in ixp_prefixes.iteritems() ]\n",
    "\n",
    "        ixp, reach = zip(*tmp)\n",
    "        next_ixp = ixp[np.argmax(reach)]\n",
    "        next_reach = np.max(reach)\n",
    "\n",
    "        next_reach = get_additional_IXP_reach(pyt, ixp_prefixes[next_ixp], number_ips_reached)\n",
    "\n",
    "        tmp = [ entry[0] for entry in tmp if entry[1] >= next_reach ]\n",
    "        tmp = [ (ixp, get_additional_IXP_reach(pyt, ixp_prefixes[ixp], number_ips_reached)) for ixp in tmp]\n",
    "\n",
    "        ixp, reach = zip(*tmp)\n",
    "        next_ixp = ixp[np.argmax(reach)]\n",
    "        next_reach = np.max(reach)\n",
    "        number_ips_reached += next_reach\n",
    "\n",
    "        for pfx in ixp_prefixes[next_ixp]:\n",
    "            pyt[pfx] = None\n",
    "\n",
    "        res.append((next_ixp, next_reach))\n",
    "        del(ixp_prefixes[next_ixp])\n",
    "\n",
    "        ixp_prefixes = { ixp: remove_pfxes(pyt, pfx_list) for ixp, pfx_list in ixp_prefixes.iteritems() }\n",
    "        \n",
    "    return (ts['ts'], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "res_ixp_with_t1 = dict(pool.map(calc_IXP_reachability, ts_merge))\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict([(k, pd.Series([x[1] for x in v])) for k, v in res_ixp_with_t1.items()]))\n",
    "df = df.cumsum(axis='index')\n",
    "df.columns = [ v.strftime('%Y') for v in df.columns ]\n",
    "df.index = df.index + 1\n",
    "\n",
    "display(df.head())\n",
    "df.to_csv('../csvs/ixps-with-t1-cumulative-reach.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', len(res_ixp_with_t1)):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    for (ts, values) in sorted(res_ixp_with_t1.iteritems(), key=operator.itemgetter(0)):\n",
    "        ax.plot(np.cumsum(zip(*values)[1]), label=ts.strftime('%Y'))\n",
    "        \n",
    "    ax.set_xscale('log')\n",
    "    ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "    ax.legend(ncol=6)\n",
    "    \n",
    "    ax.set_ylabel('# of reachable IPs')\n",
    "    ax.set_xlabel('# of IXPs')\n",
    "    \n",
    "plt.savefig('../figures/ixp-cumulative-reachability-per-year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IXP reachbility without T1 ASes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_IXP_reachability(ts):\n",
    "    # ts, tqdm_pos = params\n",
    "    \n",
    "    ixp_customer_cone_asn = {}\n",
    "\n",
    "    for ixp, member_asn in ixp_member_asn[ts['ixp_member_asn']].iteritems():\n",
    "        reachable_asn = []\n",
    "        for asn in member_asn:\n",
    "            if asn in t1_ases[ts['t1_ases']]:\n",
    "                continue\n",
    "            else:\n",
    "                reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            #reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "        ixp_customer_cone_asn[ixp] = set(reachable_asn)\n",
    "        \n",
    "    ixp_prefixes = { ixp: minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in ixp_asn ]))) for ixp, ixp_asn in ixp_customer_cone_asn.iteritems() }\n",
    "\n",
    "    number_ips_reached = 0\n",
    "    pyt = pytricia.PyTricia(32)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for _ in range(len(ixp_prefixes)):\n",
    "    #for _ in tqdm.tnrange(len(ixp_prefixes), desc=str(ts['ts'].year), position=tqdm_pos):\n",
    "        tmp = [ (ixp, get_number_IPs_from_prefixes(pfx_list)) for ixp, pfx_list in ixp_prefixes.iteritems() ]\n",
    "\n",
    "        ixp, reach = zip(*tmp)\n",
    "        next_ixp = ixp[np.argmax(reach)]\n",
    "        next_reach = np.max(reach)\n",
    "\n",
    "        next_reach = get_additional_IXP_reach(pyt, ixp_prefixes[next_ixp], number_ips_reached)\n",
    "\n",
    "        tmp = [ entry[0] for entry in tmp if entry[1] >= next_reach ]\n",
    "        tmp = [ (ixp, get_additional_IXP_reach(pyt, ixp_prefixes[ixp], number_ips_reached)) for ixp in tmp]\n",
    "\n",
    "        ixp, reach = zip(*tmp)\n",
    "        next_ixp = ixp[np.argmax(reach)]\n",
    "        next_reach = np.max(reach)\n",
    "        number_ips_reached += next_reach\n",
    "\n",
    "        for pfx in ixp_prefixes[next_ixp]:\n",
    "            pyt[pfx] = None\n",
    "\n",
    "        res.append((next_ixp, next_reach))\n",
    "        del(ixp_prefixes[next_ixp])\n",
    "\n",
    "        ixp_prefixes = { ixp: remove_pfxes(pyt, pfx_list) for ixp, pfx_list in ixp_prefixes.iteritems() }\n",
    "        \n",
    "    return (ts['ts'], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "res_ixp_without_t1 = dict(pool.map(calc_IXP_reachability, ts_merge))\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', len(res_ixp_without_t1)):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    for (ts, values) in sorted(res_ixp_without_t1.iteritems(), key=operator.itemgetter(0)):\n",
    "        ax.plot(np.cumsum(zip(*values)[1]), label=ts.strftime('%Y'))\n",
    "        \n",
    "    ax.set_xscale('log')\n",
    "    ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "    ax.legend(ncol=6)\n",
    "    \n",
    "    ax.set_ylabel('# of reachable IPs')\n",
    "    ax.set_xlabel('# of IXPs')\n",
    "    \n",
    "plt.savefig('../figures/ixp-cumulative-reachability-without-t1-per-year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fuck():\n",
    "    for ts, fuck in sorted(res_ixp_without_t1.iteritems()):\n",
    "        ixp_null_contribution = [ixp for (ixp, count) in fuck if count == 0]\n",
    "        ixp_contribution = [ixp for (ixp, count) in fuck if count > 0]\n",
    "        share_null_contribution = len(ixp_null_contribution)/len(fuck)\n",
    "        yield(dict(ts=ts, ixp_null_contribution=len(ixp_null_contribution), ixp_contribution=len(ixp_contribution), share_null_contribution=share_null_contribution, share_contribution=1-share_null_contribution))\n",
    "        \n",
    "df_fuck = pd.DataFrame.from_records(do_fuck())\n",
    "df_fuck['share_contribution'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict([(k, pd.Series([x[1] for x in v])) for k, v in res_ixp_without_t1.items()]))\n",
    "df = df.cumsum(axis='index')\n",
    "df.columns = [ v.strftime('%Y') for v in df.columns ]\n",
    "df.index = df.index + 1\n",
    "\n",
    "for ts in sorted(res_all_reach['all_reach']):\n",
    "    df['all_reach_%s' % ts.strftime('%Y')] = res_all_reach['all_reach'][ts]\n",
    "    \n",
    "for ts in sorted(res_all_reach['t1_cc_reach']):\n",
    "    df['t1_cc_reach_%s' % ts.strftime('%Y')] = res_all_reach['t1_cc_reach'][ts]\n",
    "\n",
    "display(df.head())\n",
    "df.to_csv('../csvs/ixps-without-t1-cumulative-reach.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachability vs. Redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXP_reach(ixp, ts):\n",
    "    reachable_asn = []\n",
    "    for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "        if asn in t1_ases[ts['t1_ases']]:\n",
    "            continue\n",
    "        else:\n",
    "            reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            \n",
    "    pfxes = minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in reachable_asn] )))\n",
    "    \n",
    "    return get_number_IPs_from_prefixes(pfxes)\n",
    "\n",
    "def do_calc(ts):\n",
    "    v = [ (additional_reach, get_IXP_reach(ixp, ts)) for (ixp, additional_reach) in res_ixp_without_t1[ts['ts']] ]\n",
    "    return (ts['ts'], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "res_reach_redundancy = dict(pool.map(do_calc, ts_merge))\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = {ts['ts']: (additional_reach, get_IXP_reach(ixp, ts) - additional_reach) for ts in ts_merge for (ixp, additional_reach) in res_ixp_without_t1[ts['ts']] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_list = []\n",
    "for ts, x in sorted(res_reach_redundancy.iteritems()):\n",
    "    y = np.cumsum(x, axis=0)\n",
    "    sr_list.append(pd.Series(zip(*y)[0], name='%s - reachable' % ts.strftime('%Y')))\n",
    "    sr_list.append(pd.Series(zip(*y)[1], name='%s - redundant' % ts.strftime('%Y')))\n",
    "\n",
    "df = pd.DataFrame(sr_list).transpose()\n",
    "df.index = df.index + 1\n",
    "display(df.head())\n",
    "df.to_csv('../csvs/ixps-reachability-vs-redundancy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_list = []\n",
    "for ts, x in sorted(res_reach_redundancy.iteritems()):\n",
    "    y = np.cumsum(x, axis=0)\n",
    "    s1 = pd.Series(zip(*y)[0], name='%s - reachable' % ts.strftime('%Y'))\n",
    "    s2 = pd.Series(zip(*y)[1]) / s1\n",
    "    s2.name = '%s - redundant' % ts.strftime('%Y')\n",
    "    sr_list.extend([s1, s2])\n",
    "\n",
    "df = pd.DataFrame(sr_list).transpose()\n",
    "df.index = df.index + 1\n",
    "display(df.head())\n",
    "df.to_csv('../csvs/ixps-reachability-vs-relative-redundancy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', len(res_reach_redundancy)):\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    for ts, x in sorted(res_reach_redundancy.iteritems()):\n",
    "        #x = sorted(x, key=operator.itemgetter(1), reverse=True)\n",
    "        y = np.cumsum(x, axis=0)\n",
    "        s1 = pd.Series(zip(*y)[0])\n",
    "        s2 = pd.Series(zip(*y)[1]) / s1\n",
    "        ax.plot(s1, s2, label=ts.strftime('%Y'))\n",
    "        \n",
    "    ax.legend(ncol=4)\n",
    "    \n",
    "    ax.set_ylabel('# of redundant IPs')\n",
    "    ax.set_xlabel('# of reachable IPs')\n",
    "    \n",
    "    plt.savefig('../figures/reachability-vs-redundancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', len(res)):\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    for ts, x in sorted(res.iteritems()):\n",
    "        y = np.cumsum(x, axis=0)\n",
    "        a, b = zip(*y)[0], zip(*y)[1]\n",
    "        m = float(max(a))\n",
    "        a = np.divide(a, m)\n",
    "        b = np.divide(b, m)\n",
    "        ax.plot(a, b, label=ts.strftime('%Y'))\n",
    "        \n",
    "    ax.legend(ncol=4)\n",
    "    #ax.set_yscale('log')\n",
    "    #ax.set_ylim(top=10)\n",
    "    \n",
    "    ax.set_ylabel('# of redundant IPs')\n",
    "    ax.set_xlabel('# of reachable IPs')\n",
    "    \n",
    "    plt.savefig('../figures/reachability-vs-redundancy-scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for ts in ts_merge:\n",
    "    as_reach = {asn: get_number_IPs_from_prefixes(pfxes) for asn, pfxes in asn2pfx[ts['asn2pfx']].iteritems()}\n",
    "\n",
    "    tmp = itertools.chain.from_iterable(ixp_member_asn[ts['ixp_member_asn']].itervalues())\n",
    "    as_counts = collections.Counter(tmp)\n",
    "    as_ip_redundancy = {asn: as_reach.get(asn, -1) * (count-1) for asn, count in as_counts.iteritems()}\n",
    "\n",
    "    #x = {autnums.get(asn, '') + \" (\" + str(asn) + \")\": count for asn, count in as_ip_redundancy.iteritems()}\n",
    "    x = as_ip_redundancy\n",
    "    x = sorted(x.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    res[ts['ts']] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', len(ts_merge)):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "    for ts, values in sorted(res.iteritems(), key=operator.itemgetter(0)):\n",
    "        ax.plot(np.cumsum(zip(*values)[1]), label=ts.strftime('%Y'))\n",
    "        \n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_ylabel('# of redundant IPs')\n",
    "    ax.set_xlabel('# of ASes')\n",
    "    \n",
    "plt.savefig('../figures/redundancy-per-as-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{'ts': ts, 'asn': asn, 'redundancy': redundancy} for ts, values in res.iteritems() for asn, redundancy in values ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_asn = df.groupby(['asn']).agg({'redundancy': 'mean'}).sort_values('redundancy', ascending=False).rename(columns={'redundancy': 'avg_redundancy'}).reset_index().head(10).asn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pivot('ts', 'asn', 'redundancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df.columns.difference(top10_asn)].sum(axis=1)\n",
    "df = df.filter(items=top10_asn)\n",
    "df['other'] = x\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA vs. Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXP_member(ixp, ts):\n",
    "    reachable_asn = []\n",
    "    for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "        if asn in t1_ases[ts['t1_ases']]:\n",
    "            continue\n",
    "        else:\n",
    "            reachable_asn += [asn]\n",
    "    \n",
    "    return reachable_asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXP_reach(ixp, ts):\n",
    "    reachable_asn = []\n",
    "    for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "        if asn in t1_ases[ts['t1_ases']]:\n",
    "            continue\n",
    "        else:\n",
    "            reachable_asn += [asn]\n",
    "            #reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            \n",
    "    pfxes = minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in reachable_asn] )))\n",
    "    \n",
    "    return get_number_IPs_from_prefixes(pfxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXPs_reach(ixps, ts):\n",
    "    reachable_asn = []\n",
    "    for ixp in ixps:\n",
    "        for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "            if asn in t1_ases[ts['t1_ases']]:\n",
    "                continue\n",
    "            else:\n",
    "                reachable_asn += [asn]\n",
    "                #reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            \n",
    "    pfxes = minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in reachable_asn] )))\n",
    "    \n",
    "    return get_number_IPs_from_prefixes(pfxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {'date': ts, 'ixp': ixp, 'member_asn': asn}\n",
    "    for ts, ixps in ixp_member_asn.iteritems()\n",
    "    for ixp, members in ixps.iteritems()\n",
    "    for asn in members\n",
    "])\n",
    "\n",
    "df2 = pd.DataFrame([\n",
    "    {'date': ts, 'ixp': ixp, 'continent': props['region_continent']}\n",
    "    for ts, ixps in pdb_ixps.iteritems()\n",
    "    for ixp, props in ixps.iteritems()\n",
    "])\n",
    "\n",
    "df = df.merge(df2)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixp_continent_date = df.groupby(['continent', 'date']).agg({'ixp': lambda x: set(x)}).to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_comparison = []\n",
    "for ts in tqdm.tqdm_notebook(ts_merge):\n",
    "    for continent in ['Europe', 'North America']:\n",
    "        ixps = ixp_continent_date[(continent, ts['ixp_member_asn'])]['ixp']\n",
    "        reach = sum(get_IXP_reach(ixp, ts) for ixp in ixps)\n",
    "        unique_reach = get_IXPs_reach(ixps, ts)\n",
    "        res_comparison.append(dict(ts=ts['ts'], continent=continent, unique_reach=unique_reach, reach=reach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame(res_comparison)\n",
    "df_plot['ratio'] = df_plot['reach'] / df_plot['unique_reach']\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df_plot.pivot(index='ts', columns='continent')\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_comparison = []\n",
    "for ts in tqdm.tqdm_notebook(ts_merge):\n",
    "    for continent in ['Europe', 'North America']:\n",
    "        ixps = ixp_continent_date[(continent, ts['ixp_member_asn'])]['ixp']\n",
    "        for ixp in ixps:\n",
    "            reach = get_IXP_reach(ixp, ts)\n",
    "            res_comparison.append(dict(ts=ts['ts'], continent=continent, reach=reach, ixp=ixp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res_comparison)\n",
    "df['ts'] = df['ts'].apply(lambda ts: ts.strftime('%Y'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('continent').boxplot(column='reach', by='ts', whis='range', figsize=figsize_full, rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ixp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_comparison = []\n",
    "for ts in tqdm.tqdm_notebook(ts_merge):\n",
    "    for continent in ['Africa', 'Asia Pacific', 'Australia', 'Europe', 'Middle East', 'North America', 'South America']:\n",
    "        ixps = ixp_continent_date.get((continent, ts['ixp_member_asn']), dict(ixp=[]))['ixp']\n",
    "        ixp_member = [get_IXP_member(ixp, ts) for ixp in ixps]\n",
    "        ixp_member = list(set(itertools.chain(*ixp_member)))\n",
    "        ixp_member_pfx_list = [ asn2pfx[ts['asn2pfx']].get(ixp_asn, []) for ixp_asn in ixp_member ]\n",
    "        for ixp_member_pfx in ixp_member_pfx_list:\n",
    "            reach = get_number_IPs_from_prefixes(ixp_member_pfx)\n",
    "            res_comparison.append(dict(ts=ts['ts'], continent=continent, reach=reach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res_comparison)\n",
    "df['ts'] = df['ts'].apply(lambda ts: ts.strftime('%Y'))\n",
    "df.groupby(['continent', 'ts']).agg({'reach': 'describe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../csvs/ixp-gini-data-for-ignacio.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('continent').boxplot(column='reach', by='ts', whis='range', figsize=figsize_full, rot=90)\n",
    "#ax[0].set_ylim([0, 200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('continent').boxplot(column='reach', by='ts', figsize=figsize_full, rot=90)\n",
    "ax[0].set_ylim([0, 200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df[df['ts'].isin(['2006', '2008', '2010', '2012', '2014', '2016'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_plot.groupby('continent').boxplot(column='reach', by='ts', figsize=(10, 3), rot=90, layout=(1, 7))\n",
    "\n",
    "ax[0].set_ylim([0, 300000])\n",
    "plt.tight_layout()\n",
    "plt.suptitle('');\n",
    "plt.savefig('../figures/boxplot-all-regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}\n",
    "for continent in df['continent'].unique():\n",
    "    for ts in df['ts'].unique():\n",
    "        d2[continent + '/' + ts] = df[(df['continent'] == continent) & (df['ts'] == ts)]['reach'].reset_index(drop=True)\n",
    "pd.DataFrame(d2).fillna('?').to_csv('../csvs/boxplot-ixps-all-regions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA vs. Europe vs. Rest of the World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXP_member(ixp, ts):\n",
    "    reachable_asn = []\n",
    "    for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "        if asn in t1_ases[ts['t1_ases']]:\n",
    "            continue\n",
    "        else:\n",
    "            reachable_asn += [asn]\n",
    "    \n",
    "    return reachable_asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXP_reach(ixp, ts):\n",
    "    reachable_asn = []\n",
    "    for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "        if asn in t1_ases[ts['t1_ases']]:\n",
    "            continue\n",
    "        else:\n",
    "            #reachable_asn += [asn]\n",
    "            reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            \n",
    "    pfxes = minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in reachable_asn] )))\n",
    "    \n",
    "    return get_number_IPs_from_prefixes(pfxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXPs_reach(ixps, ts):\n",
    "    reachable_asn = []\n",
    "    for ixp in ixps:\n",
    "        for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "            if asn in t1_ases[ts['t1_ases']]:\n",
    "                continue\n",
    "            else:\n",
    "                #reachable_asn += [asn]\n",
    "                reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            \n",
    "    pfxes = minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in reachable_asn] )))\n",
    "    \n",
    "    return get_number_IPs_from_prefixes(pfxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IXP_prefixes_with_customer_cone(ixp, ts):\n",
    "    reachable_asn = []\n",
    "    for asn in ixp_member_asn[ts['ixp_member_asn']][ixp]:\n",
    "        if asn in t1_ases[ts['t1_ases']]:\n",
    "            continue\n",
    "        else:\n",
    "            #reachable_asn += [asn]\n",
    "            reachable_asn += customer_cones[ts['customer_cones']].get(asn, [])\n",
    "            \n",
    "    pfxes = minimise_pfx_list(set(itertools.chain( *[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in reachable_asn] )))\n",
    "    \n",
    "    return pfxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {'date': ts, 'ixp': ixp, 'member_asn': asn}\n",
    "    for ts, ixps in ixp_member_asn.iteritems()\n",
    "    for ixp, members in ixps.iteritems()\n",
    "    for asn in members\n",
    "])\n",
    "\n",
    "df2 = pd.DataFrame([\n",
    "    {'date': ts, 'ixp': ixp, 'continent': props['region_continent']}\n",
    "    for ts, ixps in pdb_ixps.iteritems()\n",
    "    for ixp, props in ixps.iteritems()\n",
    "])\n",
    "\n",
    "df = df.merge(df2)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixp_continent_date = df.groupby(['continent', 'date']).agg({'ixp': lambda x: set(x)}).to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([ a for (a, b) in ixp_continent_date.iterkeys() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_comparison = []\n",
    "for ts in tqdm.tqdm_notebook(ts_merge):\n",
    "    for continent in ['Africa', 'Asia Pacific', 'Australia', 'Europe', 'Middle East', 'North America', 'South America']:\n",
    "        ixps = ixp_continent_date.get((continent, ts['ixp_member_asn']), dict(ixp=[]))['ixp']\n",
    "        prefixes_l = [get_IXP_prefixes_with_customer_cone(ixp, ts) for ixp in ixps]\n",
    "        prefixes = list(itertools.chain(*prefixes_l))\n",
    "        continent_reach = get_number_IPs_from_prefixes(prefixes)\n",
    "        res_comparison.append(dict(ts=ts['ts'], continent=continent, continent_reach=continent_reach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_reach = pd.DataFrame(res_all_reach)\n",
    "df_all_reach = df_all_reach.reset_index()\n",
    "df_all_reach = df_all_reach[df_all_reach['index'] >= '2008-01-01'].copy()\n",
    "df_all_reach['index'] = df_all_reach['index'].apply(lambda ts: ts.strftime('%Y'))\n",
    "df_all_reach = df_all_reach.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.DataFrame(res_comparison)\n",
    "exp = exp[exp['ts'] >= '2008-01-01'].copy()\n",
    "exp['ts'] = exp['ts'].apply(lambda ts: ts.strftime('%Y'))\n",
    "exp = exp.pivot(index='ts', columns='continent', values='continent_reach')\n",
    "exp = pd.concat([exp, df_all_reach], axis='columns')\n",
    "\n",
    "display(exp)\n",
    "exp.to_csv('../csvs/ixp-reach-per-year-per-region.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IXP Jaccard Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ts_merge[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_number_ips = { asn: get_number_IPs_from_prefixes(pfxes) for asn, pfxes in asn2pfx[ts['asn2pfx']].iteritems() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ixp_jaccard_distance(ixp_a, ixp_b, ts):\n",
    "    \n",
    "    # get ASN in customer cone of IXP A\n",
    "    ixp_a_customer_cone_asn = set()\n",
    "    for member_asn in ixp_member_asn[ts['ixp_member_asn']][ixp_a]:\n",
    "        ixp_a_customer_cone_asn.update(customer_cones[ts['customer_cones']].get(member_asn, [member_asn]))\n",
    "    ixp_a_customer_cone_asn = set(ixp_a_customer_cone_asn)\n",
    "        \n",
    "    # get ASN in customer cone of IXP B\n",
    "    ixp_b_customer_cone_asn = set()\n",
    "    for member_asn in ixp_member_asn[ts['ixp_member_asn']][ixp_b]:\n",
    "        ixp_b_customer_cone_asn.update(customer_cones[ts['customer_cones']].get(member_asn, [member_asn]))\n",
    "    ixp_b_customer_cone_asn = set(ixp_b_customer_cone_asn)\n",
    "    \n",
    "    intersection_asn = ixp_a_customer_cone_asn & ixp_b_customer_cone_asn\n",
    "    union_asn = ixp_a_customer_cone_asn | ixp_b_customer_cone_asn\n",
    "    \n",
    "    intersection_ips = sum( [asn_number_ips.get(asn, 0) for asn in intersection_asn] )\n",
    "    union_ips = sum( [asn_number_ips.get(asn, 0) for asn in union_asn] )\n",
    "    \n",
    "    #intersection_ips = get_number_IPs_from_prefixes(itertools.chain(*[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in intersection_asn ]))\n",
    "    #union_ips = get_number_IPs_from_prefixes(itertools.chain(*[ asn2pfx[ts['asn2pfx']].get(asn, []) for asn in union_asn ]))\n",
    "    \n",
    "    try:\n",
    "        res = 1.0 * intersection_ips / union_ips\n",
    "    except ZeroDivisionError:\n",
    "        res = 0 \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ixp_jaccard_distance('DE-CIX Frankfurt', 'AMS-IX', ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = {\n",
    "    ixp_a: {\n",
    "        ixp_b: ixp_jaccard_distance(ixp_a, ixp_b, ts) for ixp_b in ixp_member_asn[ts['ixp_member_asn']].iterkeys()\n",
    "    } for ixp_a in ixp_member_asn[ts['ixp_member_asn']].iterkeys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "sns.heatmap(pd.DataFrame(x), xticklabels=False, yticklabels=False, ax=ax)\n",
    "ax.set_ylabel('IXPs')\n",
    "ax.set_xlabel('IXPs')\n",
    "\n",
    "plt.savefig('../figures/ixp-ip-customer-cone-similarity-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two - Traceroutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://ixp_history:ixp_history@localhost:27017/ixp_history')\n",
    "db = client['ixp_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distinct_iplane = db['agg_counts_ip_hops_as_hops_ixp_hops_plus_src'].find({'_id.type': 'iplane'}).distinct('_id.src')\n",
    "distinct_caida = db['agg_counts_ip_hops_as_hops_ixp_hops_plus_src'].find({'_id.type': 'caida-ark'}).distinct('_id.src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(distinct_iplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(distinct_caida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    collection = db['all_traceroutes_filtered']\n",
    "    for t in collection.distinct('type'):\n",
    "        for date in collection.distinct('date', query={'type': t}):\n",
    "            for src in collection.distinct('src', query={'type': t, 'date': date}):\n",
    "                yield(dict(type=t, date=date, src=src))\n",
    "\n",
    "df = pd.DataFrame.from_records(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'type']).agg({'src': 'nunique'})\n",
    "gb = gb.reset_index()\n",
    "gb.groupby(['type']).agg({'src': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(node):\n",
    "    if node in pl_nodes_countries:\n",
    "        return pl_nodes_countries[node]\n",
    "    if len(node) == 6:\n",
    "        node = node.split('-')\n",
    "        return node[1]\n",
    "    if len(node) == 7:\n",
    "        node = node.split('-')\n",
    "        return node[1]\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = df['src'].apply(get_country)\n",
    "df.to_csv('../csvs/node-data-for-ignacio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in pl_nodes_countries.iteritems():\n",
    "    if len(v) != 2:\n",
    "        print k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import incf.countryutils.transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caida_countries = [ node_name.split('-')[1] for node_name in distinct_caida ]\n",
    "caida_countries = [u'gb' if cc == 'uk' else cc for cc in caida_countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('../code/traceroutes/pl_nodes_countries_fix.json.bz2') as f:\n",
    "    pl_nodes_countries = json.load(f)\n",
    "pl_nodes_countries['pl1.tailab.eu'] = 'fr'\n",
    "pl_nodes_countries['pl2.tailab.eu'] = 'fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records((dict(domain=k, cc=v) for k, v in pl_nodes_countries.iteritems()))\n",
    "df['tld'] = df['domain'].apply(lambda d: d.split('.')[-1])\n",
    "df['tld_match'] = df['tld'] == df['cc']\n",
    "df = df[df['domain'].isin(distinct_iplane)]\n",
    "print(df['tld_match'].value_counts())\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['tld_match']]['tld'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cc'] == 'un']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplane_countries = [pl_nodes_countries[node_name] for node_name in distinct_iplane]\n",
    "iplane_countries = [u'gb' if cc == 'uk' else cc for cc in iplane_countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(caida_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(caida_countries).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(iplane_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(iplane_countries).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = set(caida_countries + [cc for cc in iplane_countries if len(cc) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplane_continents = map(incf.countryutils.transformations.cca_to_ctn, set((cc for cc in iplane_countries if len(cc) == 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caida_continents = map(incf.countryutils.transformations.cca_to_ctn, set(caida_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_continents = map(incf.countryutils.transformations.cca_to_ctn, all_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(iplane_continents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(caida_continents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(all_continents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(collections.Counter(all_continents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    for con in ['EU', 'NA', 'SA', 'AF', 'AS', 'OC']:\n",
    "        con_countries = incf.countryutils.transformations.ctca2_to_ccn(con)\n",
    "        con_countries = map(incf.countryutils.transformations.ccn_to_cca3, con_countries)\n",
    "        con = {'EU': 'Europe', 'NA': 'North America', 'SA': 'South America', 'AF': 'Africa', 'AS': 'Asia', 'OC': 'Oceania'}[con]\n",
    "        yield(dict(continent=con,total_count=len(con_countries)))\n",
    "        \n",
    "df_con_counts = pd.DataFrame.from_records(get_data())\n",
    "df_con_counts = df_con_counts.set_index('continent')\n",
    "df_con_counts['hit_count'] = pd.Series(collections.Counter(all_continents))\n",
    "df_con_counts['ratio'] = df_con_counts['hit_count'] / df_con_counts['total_count'] * 100\n",
    "display(df_con_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.read_csv('https://raw.githubusercontent.com/datasets/population/master/data/population.csv')\n",
    "display(df_pop.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.read_json('https://raw.githubusercontent.com/lorey/list-of-countries/master/json/countries.json')\n",
    "display(df_pop.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop(cca3):\n",
    "    return df_pop[(df_pop['Country Code'] == cca3) & (df_pop['Year'] == 2016)]['Value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traceroutes = df_traceroutes.merge(df_pop.loc[:, ['alpha_3', 'population']], left_on='cca3', right_on='alpha_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traceroutes = pd.DataFrame(pd.Series(list(all_countries), name='country'))\n",
    "df_traceroutes['continent'] = df_traceroutes['country'].apply(incf.countryutils.transformations.cca_to_ctn)\n",
    "df_traceroutes['cca3'] = df_traceroutes['country'].apply(lambda cc: incf.countryutils.transformations.ccn_to_cca3(incf.countryutils.transformations.cc_to_ccn(cc)))\n",
    "\n",
    "len_old = len(df_traceroutes)\n",
    "df_traceroutes = df_traceroutes.merge(df_pop.loc[:, ['alpha_3', 'population']], left_on='cca3', right_on='alpha_3')\n",
    "assert len(df_traceroutes) == len_old\n",
    "display(df_traceroutes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    for con in ['EU', 'NA', 'SA', 'AF', 'AS', 'OC']:\n",
    "        con_countries = incf.countryutils.transformations.ctca2_to_ccn(con)\n",
    "        con_countries = map(incf.countryutils.transformations.ccn_to_cca3, con_countries)\n",
    "        pops = []\n",
    "        for c in con_countries:\n",
    "            try:\n",
    "                pops.append(df_pop[df_pop['alpha_3'] == c]['population'].values[0])\n",
    "            except:\n",
    "                print(c)\n",
    "        con = {'EU': 'Europe', 'NA': 'North America', 'SA': 'South America', 'AF': 'Africa', 'AS': 'Asia', 'OC': 'Oceania'}[con]\n",
    "        yield(dict(continent=con,total_population=sum(pops)))\n",
    "        \n",
    "df_con_pop = pd.DataFrame.from_records(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_traceroutes.groupby('continent').agg({'population': 'sum'}).reset_index().merge(df_con_pop)\n",
    "df_res['ratio'] = df_res['population'] / df_res['total_population'] * 100\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = db['all_traceroutes'].distinct('date', {'type': 'iplane'})\n",
    "dates_differences = [(d2 - d1).days for d1, d2 in zip(dates, dates[1:])]\n",
    "\n",
    "fix, ax = plt.subplots(figsize=figsize_half)\n",
    "ax.plot(dates[1:], dates_differences)\n",
    "ax.set_title('iPlane date differences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = db['all_traceroutes'].distinct('date', {'type': 'iplane'})\n",
    "sorted(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = db['all_traceroutes'].distinct('date', {'type': 'caida-ark'})\n",
    "dates_differences = [(d2 - d1).days for d1, d2 in zip(dates, dates[1:])]\n",
    "\n",
    "fix, ax = plt.subplots(figsize=figsize_half)\n",
    "ax.plot(dates[1:], dates_differences)\n",
    "ax.set_title('Caida Ark date differences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db['agg_counts_dst_ips_vs_asterisk_hops_vs_ixp_hops'].find({}))\n",
    "df = df.join(pd.DataFrame(df[\"_id\"].values.tolist(), index=df.index))\n",
    "df = df.drop(columns=['_id'])\n",
    "df['count'] = df['count'].astype(int)\n",
    "print('Length:', len(df))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ (df.numberOfdstips == 1) & (df.numberOfIXPhops <= 1) ].groupby(['type', 'numberOfasteriskhops']).agg({'count': 'sum'}).unstack('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['type']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numberOfdstips == 0].groupby(['type']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numberOfdstips > 1].groupby(['type']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numberOfdstips == 1]['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numberOfdstips == 1].groupby(['type']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numberOfdstips == 1].groupby(['type']).agg({'count': 'sum'}) / df.groupby(['type']).agg({'count': 'sum'}) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ]['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ].groupby(['type']).agg({'count': 'sum'}))\n",
    "display(df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ].groupby(['type']).agg({'count': 'sum'}) / df.groupby(['type']).agg({'count': 'sum'}) * 100)\n",
    "display(df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ].groupby(['type']).agg({'count': 'sum'}) - df[df.numberOfdstips == 1].groupby(['type']).agg({'count': 'sum'}))\n",
    "display((df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ].groupby(['type']).agg({'count': 'sum'}) - df[df.numberOfdstips == 1].groupby(['type']).agg({'count': 'sum'})) / df.groupby(['type']).agg({'count': 'sum'}) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) & (df.numberOfIXPhops <= 1)]['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) & (df.numberOfIXPhops <= 1)].groupby(['type']).agg({'count': 'sum'}))\n",
    "display(df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) & (df.numberOfIXPhops <= 1)].groupby(['type']).agg({'count': 'sum'}) / df.groupby(['type']).agg({'count': 'sum'}) * 100)\n",
    "display(df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) & (df.numberOfIXPhops <= 1)].groupby(['type']).agg({'count': 'sum'}) - df[(df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ].groupby(['type']).agg({'count': 'sum'}))\n",
    "display((df[ (df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) & (df.numberOfIXPhops <= 1)].groupby(['type']).agg({'count': 'sum'}) - df[(df.numberOfdstips == 1) & (df.numberOfasteriskhops <= 1) ].groupby(['type']).agg({'count': 'sum'}) )/ df.groupby(['type']).agg({'count': 'sum'}) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type').agg({'date': ['min', 'max']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type').agg({'date': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceroute Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ips = {ts: get_number_IPs_from_prefixes(list(itertools.chain(*[pfxes for _, pfxes in values.iteritems()]))) for ts, values in tqdm.tqdm_notebook(asn2pfx.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "asn_count = {k: len(set(v.iterkeys())) for k, v in asn2pfx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame.from_records(db['y_combined_type_date_dst_asn'].find({}, {'date':  1, 'type': 1, 'dst_asn': 1, '_id': 0}))\n",
    "df = df.dropna(subset=['dst_asn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ips_from_asn_list(row):\n",
    "    next_date = min(asn2pfx.keys(), key=lambda x: abs(row['date_'] - x))\n",
    "    pfx = itertools.chain(*[asn2pfx[next_date].get(asn, []) for asn in map(int, row['dst_asn_unique'])])\n",
    "    return get_number_IPs_from_prefixes(pfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gb = df.groupby(['type', 'date']).agg({'dst_asn': ['unique', 'nunique']}).reset_index()\n",
    "gb.columns =  ['_'.join(col).strip() for col in gb.columns.values]\n",
    "gb['announced_ips'] = gb[['date_', 'dst_asn_unique']].apply(get_ips_from_asn_list, axis=1)\n",
    "gb['active_ips'] = gb['date_'].apply(lambda x: all_ips[min(all_ips.keys(), key=lambda y: abs(x - y))])\n",
    "gb['covered_ips'] = gb['announced_ips'] / gb['active_ips']\n",
    "gb['active_asn'] = gb['date_'].apply(lambda x: asn_count[min(asn_count.keys(), key=lambda y: abs(x - y))])\n",
    "gb['covered_asn'] = gb['dst_asn_nunique'] / gb['active_asn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = gb.drop('dst_asn_unique', axis='columns').copy()\n",
    "display(df_a.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.groupby('type_').agg({'covered_asn': 'mean', 'covered_ips': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "gb.pivot_table(index='date_', columns='type_', values=['covered_asn', 'covered_ips']).plot(ax=ax)\n",
    "ax.set_title('Covered ASN / IPs')\n",
    "\n",
    "plt.savefig('../figures/traceroute-covered-asn-covered-ips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gb = df.groupby(['date']).agg({'dst_asn': ['unique', 'nunique']}).reset_index()\n",
    "gb.columns =  ['_'.join(col).strip() for col in gb.columns.values]\n",
    "gb['announced_ips'] = gb[['date_', 'dst_asn_unique']].apply(get_ips_from_asn_list, axis=1)\n",
    "gb['active_ips'] = gb['date_'].apply(lambda x: all_ips[min(all_ips.keys(), key=lambda y: abs(x - y))])\n",
    "gb['covered_ips'] = gb['announced_ips'] / gb['active_ips']\n",
    "gb['active_asn'] = gb['date_'].apply(lambda x: asn_count[min(asn_count.keys(), key=lambda y: abs(x - y))])\n",
    "gb['covered_asn'] = gb['dst_asn_nunique'] / gb['active_asn']\n",
    "gb['type_'] = 'total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = gb.drop('dst_asn_unique', axis='columns').copy()\n",
    "display(df_b.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type_').agg({'covered_asn': 'mean', 'covered_ips': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "gb.pivot_table(index='date_', columns=None, values=['covered_asn', 'covered_ips']).plot(ax=ax)\n",
    "ax.set_title('Covered ASN / IPs')\n",
    "\n",
    "plt.savefig('../figures/traceroute-covered-asn-covered-ips-both-sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_a, df_b])\n",
    "exp = df.pivot_table(index='date_', columns='type_', values=['covered_asn', 'covered_ips'])\n",
    "exp.columns = ['/'.join(map(str,c)[::-1]) for c in exp.columns]\n",
    "exp.to_csv('../csvs/traceroute-asn-coverage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "df.pivot_table(index='date_', columns='type_', values=['covered_asn', 'covered_ips']).plot(ax=ax)\n",
    "\n",
    "plt.savefig('../figures/traceroute-asn-coverage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db['agg_counts_dst_ips_vs_asterisk_hops_vs_ixp_hops'].find({}))\n",
    "df = df.join(pd.DataFrame(df[\"_id\"].values.tolist(), index=df.index))\n",
    "df = df.drop(columns=['_id'])\n",
    "df['count'] = df['count'].astype(int)\n",
    "print('Length:', len(df))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttype in ['iplane', 'caida-ark']:\n",
    "\n",
    "    gb = df[df['type'] == ttype].groupby(['numberOfasteriskhops', 'numberOfdstips']).agg({'count': 'sum'}).reset_index()\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    sns.heatmap(gb.pivot(index='numberOfasteriskhops', columns='numberOfdstips',  values='count'), ax=ax, )\n",
    "    ax.set_title(ttype)\n",
    "    plt.savefig('../figures/heatmap-counts-dstips-vs-asteriskhops-%s' % ttype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IXP vs. non-IXP traceroutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db['agg_counts_dst_ips_vs_asterisk_hops_vs_ixp_hops'].find({}))\n",
    "df = df.join(pd.DataFrame(df[\"_id\"].values.tolist(), index=df.index))\n",
    "df = df.drop(columns=['_id'])\n",
    "df['count'] = df['count'].astype(int)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttype in ['iplane', 'caida-ark']:\n",
    "\n",
    "    gb = df[df['type'] == ttype].groupby(['date', df['numberOfIXPhops'] > 0])\n",
    "    gb = gb.agg({'count': 'sum'}).unstack()\n",
    "    gb.columns = gb.columns.droplevel(0)\n",
    "    gb.columns.name = 'IXP-traceroute'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "    gb.plot(kind='bar', stacked=True, ax=ax)\n",
    "    ax.set_title(ttype)\n",
    "    ax.xaxis.set_ticklabels(gb.index.strftime('%Y-%m-%d'));\n",
    "\n",
    "    plt.savefig('../figures/traceroute-ixp-non-ixp-%s' % ttype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[ \n",
    "    (df.numberOfdstips == 1) &\n",
    "    (df.numberOfasteriskhops <= 1) &\n",
    "    (df.numberOfIXPhops <= 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupby(['type', 'numberOfIXPhops']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupby(['type']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupby(['numberOfIXPhops']).agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupby('type').agg({'date': 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.groupby(['type', 'numberOfIXPhops', 'date']).agg({'count': 'sum'}).groupby(['type', 'numberOfIXPhops']).agg({'count': 'mean'}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttype in ['iplane', 'caida-ark']:\n",
    "    gb = df_filtered[df_filtered['type'] == ttype].groupby(['date', df['numberOfIXPhops'] > 0])\n",
    "    gb = gb.agg({'count': 'sum'}).unstack()\n",
    "    gb.columns = gb.columns.droplevel(0)\n",
    "    gb.columns.name = 'IXP-traceroute'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "    #ax.plot(kind='bar', stacked=True, ax=ax)\n",
    "    ax.stackplot(gb.index, gb[False], gb[True])\n",
    "    \n",
    "    ax.set_title(ttype)\n",
    "    #ax.xaxis.set_ticklabels(gb.index.strftime('%Y-%m-%d'));\n",
    "\n",
    "    plt.savefig('../figures/traceroute-ixp-non-ixp-filtered-%s' % ttype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "for ttype in ['iplane', 'caida-ark']:\n",
    "    gb = df_filtered[df_filtered['type'] == ttype].groupby(['date', df['numberOfIXPhops'] > 0, 'type'])\n",
    "    gb = gb.agg({'count': 'sum'}).unstack().unstack()\n",
    "    gb.name = ttype\n",
    "    #gb = gb.div(gb.sum(axis=1), axis=0)\n",
    "    #gb.columns = gb.columns.droplevel(0)\n",
    "    #gb.columns.name = 'IXP-traceroute'\n",
    "    \n",
    "    x = gb.plot(kind='line', stacked=False, ax=ax)\n",
    "    #ax.set_title(ttype)\n",
    "    \n",
    "    #display(gb.head())\n",
    "    #display(gb.tail())\n",
    "    \n",
    "    #ax.xaxis.set_ticklabels(gb.index.strftime('%Y-%m-%d'));\n",
    "\n",
    "plt.savefig('../figures/traceroute-ixp-non-ixp-filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df_filtered.groupby(['type', 'date', 'numberOfIXPhops'])\n",
    "exp = exp.agg({'count': 'sum'}).unstack('numberOfIXPhops')\n",
    "exp = exp.div(exp.sum(axis=1), axis=0)\n",
    "exp = exp.unstack('type')\n",
    "exp.columns = exp.columns.droplevel(0)\n",
    "exp.columns = ['/'.join(map(str,c)[::-1]) for c in exp.columns]\n",
    "\n",
    "exp.head()\n",
    "exp.to_csv('../csvs/traceroute-ixp-non-ixp-filtered-relative-share.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttype in ['iplane', 'caida-ark']:\n",
    "    gb = df_filtered[df_filtered['type'] == ttype].groupby(['date', df['numberOfIXPhops'] > 0])\n",
    "    gb = gb.agg({'count': 'sum'}).unstack()\n",
    "    gb = gb.div(gb.sum(axis=1), axis=0)\n",
    "    gb.columns = gb.columns.droplevel(0)\n",
    "    gb.columns.name = 'IXP-traceroute'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "    ax.stackplot(gb.index, gb[False], gb[True])\n",
    "    ax.set_title(ttype)\n",
    "    #ax.xaxis.set_ticklabels(gb.index.strftime('%Y-%m-%d'));\n",
    "\n",
    "    #plt.savefig('../figures/traceroute-ixp-non-ixp-filtered-relative-share-%s' % ttype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average AS and IP hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame.from_records(db['y_combined_type_date_ixp_hops'].find({}, {'_id': 0}))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ixp_status'] = df['ixp_hops'].apply(lambda x: 'IXP' if x > 0 else 'No IXP')\n",
    "gb = df.groupby(['date', 'ixp_status', 'type']).agg({'ip_hops_mean': 'sum', 'as_hops_mean': 'sum'})\n",
    "gb = gb.unstack().unstack()\n",
    "gb.columns.names = ['', 'Source', 'Through-IXP']\n",
    "gb.columns = ['/'.join(map(str,c)) for c in gb.columns]\n",
    "display(gb.head())\n",
    "\n",
    "#gb = gb.fillna(0)\n",
    "gb.to_csv('../csvs/avg-ip-hops-avg-as-hops-filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette(\"colorblind\", 8):\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    gb.plot(kind='line', ax=ax, marker='o')\n",
    "    plt.savefig('../figures/avg-ip-hops-avg-as-hops-filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ixp_status'] = df['ixp_hops'].apply(lambda x: 'IXP' if x > 0 else 'No IXP')\n",
    "gb = df.groupby(['date', 'ixp_status', 'type']).agg({'ip_hops_median': 'sum', 'as_hops_median': 'sum'})\n",
    "gb = gb.unstack().unstack()\n",
    "gb.columns.names = ['', 'Source', 'Through-IXP']\n",
    "gb.columns = ['/'.join(map(str,c)) for c in gb.columns]\n",
    "display(gb.head())\n",
    "\n",
    "#gb = gb.fillna(0)\n",
    "#gb.to_csv('../csvs/avg-ip-hops-avg-as-hops-filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette(\"colorblind\", 8):\n",
    "    fig, ax = plt.subplots(4, 2, figsize=figsize_a4)\n",
    "    gb[['as_hops_median/caida-ark/IXP']].plot(kind='line', ax=ax[0][0], marker='o')\n",
    "    ax[0][0].set_title('as_hops_median/caida-ark/IXP')\n",
    "    \n",
    "    gb[['as_hops_median/caida-ark/No IXP']].plot(kind='line', ax=ax[0][1], marker='o')\n",
    "    ax[0][1].set_title('as_hops_median/caida-ark/No IXP')\n",
    "    \n",
    "    gb[['as_hops_median/iplane/IXP']].plot(kind='line', ax=ax[1][0], marker='o')\n",
    "    ax[1][0].set_title('as_hops_median/iplane/IXP')\n",
    "    \n",
    "    gb[['as_hops_median/iplane/No IXP']].plot(kind='line', ax=ax[1][1], marker='o')\n",
    "    ax[1][1].set_title('as_hops_median/iplane/No IXP')\n",
    "    \n",
    "    gb[['ip_hops_median/caida-ark/IXP']].plot(kind='line', ax=ax[2][0], marker='o')\n",
    "    ax[2][0].set_title('ip_hops_median/caida-ark/IXP')\n",
    "    \n",
    "    gb[['ip_hops_median/caida-ark/No IXP']].plot(kind='line', ax=ax[2][1], marker='o')\n",
    "    ax[2][1].set_title('ip_hops_median/caida-ark/No IXP')\n",
    "    \n",
    "    gb[['ip_hops_median/iplane/IXP']].plot(kind='line', ax=ax[3][0], marker='o')\n",
    "    ax[3][0].set_title('ip_hops_median/iplane/IXP')\n",
    "    \n",
    "    gb[['ip_hops_median/iplane/No IXP']].plot(kind='line', ax=ax[3][1], marker='o')\n",
    "    ax[3][1].set_title('ip_hops_median/iplane/No IXP')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('../figures/median-as-hops-median-ip-hops-filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average = lambda x: np.average(x.values, weights=df.loc[x.index, 'n_traces'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_median(x):\n",
    "    n_counts = df.loc[x.index, 'count'].values\n",
    "    x = x.values\n",
    "    counts_reconstruct = defaultdict(lambda: 0)\n",
    "    \n",
    "    for hops, count in zip(x, n_counts):\n",
    "        counts_reconstruct[hops] += count\n",
    "        \n",
    "    return(np.median(list(collections.Counter(counts_reconstruct).elements())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ixp_status'] = df['ixp_hops'].apply(lambda x: 'IXP' if x > 0 else 'No IXP')\n",
    "gb = df.groupby(['date', 'type']).agg({'ip_hops_mean': 'mean', 'as_hops_mean': 'mean'})\n",
    "gb = gb.unstack()\n",
    "#gb.columns.names = ['', 'Source', 'Through-IXP']\n",
    "#gb.columns = ['/'.join(map(str,c)) for c in gb.columns]\n",
    "display(gb.head())\n",
    "\n",
    "#gb = gb.fillna(0)\n",
    "#gb.to_csv('../csvs/avg-ip-hops-avg-as-hops-filtered.csv')\n",
    "\n",
    "gb.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_hypergiants = pd.DataFrame.from_records(db['y_combined_type_date_ixp_hops_dst_asn'].find({'dst_asn': {'$in': hypergiant_asn}}, {'_id': 0}))\n",
    "df_hypergiants['ixp_hops'] = df_hypergiants['ixp_hops'].astype(int)\n",
    "display(df_hypergiants.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=figsize_full)\n",
    "\n",
    "df_hypergiants[df_hypergiants['type'] == 'iplane'].groupby(['date', 'type', 'ixp_hops']).agg({'n_traces': 'sum'}).unstack(['ixp_hops', 'type']).plot(ax=ax[0])\n",
    "df_hypergiants[df_hypergiants['type'] == 'caida-ark'].groupby(['date', 'type', 'ixp_hops']).agg({'n_traces': 'sum'}).unstack(['ixp_hops', 'type']).plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average = lambda x: np.average(x.values, weights=df_hypergiants.loc[x.index, 'n_traces'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "df_hypergiants[df_hypergiants['type'] == 'caida-ark'].groupby(['date', 'ixp_hops']).agg({'ip_hops_mean': weighted_average, 'as_hops_mean': weighted_average}).unstack().plot(ax=ax, marker='o')\n",
    "ax.set_title('Traces to Hypergiants')\n",
    "\n",
    "plt.savefig('../figures/mean-as-hops-ip-hops-hypergiants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df_hypergiants.groupby(['date', 'type', 'ixp_hops']).agg({'ip_hops_mean': 'mean', 'as_hops_mean': 'mean'}).unstack().unstack()\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp.to_csv('../csvs/hypergiants_as_ip_hops_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 3, figsize=(15, 15*1.414), sharex=False, sharey=False)\n",
    "axes = list(itertools.chain(*axes))\n",
    "\n",
    "for ax, hg in zip(axes, hypergiant_asn):\n",
    "    for ttype in ['iplane', 'caida-ark']:\n",
    "        df_plot = df_hypergiants[ (df_hypergiants.dst_asn == hg) & (df_hypergiants.ixp_hops == 0) & (df_hypergiants['type'] == ttype) ]\n",
    "        df_plot = df_plot.sort_values('date')\n",
    "        ax.plot(df_plot['date'].values, df_plot['as_hops_median'], label='non-IXP (%s)' % ttype)\n",
    "        ax.fill_between(df_plot['date'].values, df_plot['as_hops_q1'], df_plot['as_hops_q3'], alpha=0.15)\n",
    "\n",
    "        df_plot = df_hypergiants[ (df_hypergiants.dst_asn == hg) & (df_hypergiants.ixp_hops == 1) & (df_hypergiants['type'] == ttype) ]\n",
    "        df_plot = df_plot.sort_values('date')\n",
    "        ax.plot(df_plot['date'].values, df_plot['as_hops_median'], label='IXP (%s)' % ttype)\n",
    "        ax.fill_between(df_plot['date'].values, df_plot['as_hops_q1'], df_plot['as_hops_q3'], alpha=0.15)\n",
    "\n",
    "    ax.legend(frameon=True, loc='upper left')\n",
    "    ax.set_title('AS %s' % hg)\n",
    "    ax.set_xlim((732300.4, 736361.6))\n",
    "    ax.set_ylim((0, 10))\n",
    "    \n",
    "plt.suptitle(\"AS hops\", y=0.905);\n",
    "\n",
    "plt.savefig('../figures/hypergiant-as-hops-grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 3, figsize=(15, 15*1.414), sharex=False)\n",
    "axes = list(itertools.chain(*axes))\n",
    "\n",
    "for ax, hg in zip(axes, hypergiant_asn):\n",
    "    for ttype in ['iplane', 'caida-ark']:\n",
    "        df_plot = df_hypergiants[ (df_hypergiants.dst_asn == hg) & (df_hypergiants.ixp_hops == 0) & (df_hypergiants['type'] == ttype) ]\n",
    "        df_plot = df_plot.sort_values('date')\n",
    "        ax.plot(df_plot['date'].values, df_plot['ip_hops_median'], label='non-IXP (%s)' % ttype)\n",
    "        ax.fill_between(df_plot['date'].values, df_plot['ip_hops_q1'], df_plot['ip_hops_q3'], alpha=0.15)\n",
    "\n",
    "        df_plot = df_hypergiants[ (df_hypergiants.dst_asn == hg) & (df_hypergiants.ixp_hops == 1) & (df_hypergiants['type'] == ttype) ]\n",
    "        df_plot = df_plot.sort_values('date')\n",
    "        ax.plot(df_plot['date'].values, df_plot['ip_hops_median'], label='IXP (%s)' % ttype)\n",
    "        ax.fill_between(df_plot['date'].values, df_plot['ip_hops_q1'], df_plot['ip_hops_q3'], alpha=0.15)\n",
    "\n",
    "    ax.legend(frameon=True, loc='upper left')\n",
    "    ax.set_title('AS %s' % hg)\n",
    "    ax.set_xlim((732300.4, 736361.6))\n",
    "    ax.set_ylim((0, 25))\n",
    "    \n",
    "plt.suptitle(\"IP hops\", y=0.905);\n",
    "\n",
    "plt.savefig('../figures/hypergiant-ip-hops-grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"always\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_hypergiants[ (df_hypergiants.dst_asn == 714) & (df_hypergiants.ixp_hops == 0) ].date.values.astype(int) / 10**9\n",
    "y = df_hypergiants[ (df_hypergiants.dst_asn == 714) & (df_hypergiants.ixp_hops == 0) ].ip_hops_median.values\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, intercept + slope*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proj = {'_id': 0, 'type': 1, 'date': 1, 'dst_asn': 1, 'ixp_hops': 1, 'n_traces': 1, 'ip_hops_mean': 1, 'as_hops_mean': 1, 'ip_hops_median': 1, 'as_hops_median': 1}\n",
    "df = pd.DataFrame.from_records(db['y_combined_type_date_ixp_hops_dst_asn'].find({}, proj))\n",
    "df = df.dropna(subset=['dst_asn'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df[['type', 'n_traces']].groupby(['n_traces', 'type']).agg({'n_traces': 'count'}).unstack()\n",
    "gb.columns = gb.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "(gb.cumsum() / gb.sum()).plot(ax=ax)\n",
    "ax.set_xlim([0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "ax.set_xlim([0, 15])\n",
    "(1 - gb.cumsum() / gb.sum()).plot(ax=ax, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linreg = df[df['n_traces'] >= 10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby(['dst_asn', 'ixp_hops', 'type']).agg({'date': ['max', 'min', 'count']})\n",
    "tmp['timespan'] = tmp[('date', 'max')] - tmp[('date', 'min')]\n",
    "tmp['count'] = tmp[('date', 'count')]\n",
    "tmp = tmp.reset_index()\n",
    "tmp = tmp.drop(labels=[('date', 'max'), ('date', 'min'), ('date', 'count')], axis='columns')\n",
    "tmp.columns = tmp.columns.droplevel(1)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_timespan = tmp[['timespan']].copy()\n",
    "cdf_timespan['count'] = 1.0 / len(tmp)\n",
    "cdf_timespan = cdf_timespan.sort_values('timespan')\n",
    "cdf_timespan = cdf_timespan.groupby('timespan').agg({'count': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_timespan.index = cdf_timespan.index / pd.Timedelta('360days')\n",
    "cdf_timespan.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linreg = df.merge(tmp, on=['dst_asn', 'ixp_hops', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) == len(df_linreg), 'Length mismatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_linreg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linreg = df_linreg[df_linreg['count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linreg = df_linreg[df_linreg['timespan'] >= '180 days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_linreg_slope(group):\n",
    "    \n",
    "    if len(group) < 2:\n",
    "        print(group)\n",
    "        sys.exit(123)\n",
    "    \n",
    "    group = group.sort_values('date')\n",
    "    x = group['date'].values.astype(int) / 10**9 / 31104000.0\n",
    "    \n",
    "    y = group['as_hops_mean'].values\n",
    "    slope_as_hops_mean, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    y = group['ip_hops_mean'].values\n",
    "    slope_ip_hops_mean, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    y = group['as_hops_median'].values\n",
    "    slope_as_hops_median, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    y = group['ip_hops_median'].values\n",
    "    slope_ip_hops_median, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'slope_as_hops_mean': slope_as_hops_mean,\n",
    "        'slope_ip_hops_mean': slope_ip_hops_mean,\n",
    "        'slope_as_hops_median': slope_as_hops_median,\n",
    "        'slope_ip_hops_median': slope_ip_hops_median,\n",
    "    })\n",
    "\n",
    "gb = df_linreg.groupby(['type', 'dst_asn', 'ixp_hops']).apply(get_linreg_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slopes = gb.reset_index()\n",
    "df_slopes = df_slopes.merge(tmp, on=['dst_asn', 'ixp_hops', 'type'])\n",
    "#df_slopes = df_slopes.dropna(subset=['slope_as_hops_median'])\n",
    "#df_slopes = df_slopes.dropna(subset=['slope_ip_hops_median'])\n",
    "display(df_slopes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "ax.scatter(df_slopes['timespan'].astype(int) / 10**9 / (3600*24*360), df_slopes['slope_as_hops_mean'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_slopes[df_slopes['count'] >= 50].copy()\n",
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "ax.scatter(df_plot['timespan'].astype(int) / 10**9 / (3600*24*360), df_plot['slope_as_hops_mean'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_slopes[df_slopes['timespan'] >= '720 days'].copy()\n",
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "ax.scatter(df_plot['timespan'].astype(int) / 10**9 / (3600*24*360), df_plot['slope_as_hops_mean'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plot(field, ax):\n",
    "    series = []\n",
    "    for key, group in df_plot.groupby(['type', 'ixp_hops']):\n",
    "        group = group.sort_values(field)\n",
    "        group['sum'] = 1.0 / len(group)\n",
    "        cdf = group.groupby(field).agg({'sum': 'sum'})\n",
    "        cdf['sum'] = cdf['sum'].cumsum()\n",
    "\n",
    "        label = {'caida-ark': 'Ark', 'iplane': 'iPlane'}[key[0]]\n",
    "        label += ' - '\n",
    "        label += {0: 'No IXP', 1: 'IXP'}[key[1]]\n",
    "        ax.plot(cdf, label=label)\n",
    "        ax.set_xlabel('Rate of change (hops/year)')\n",
    "        \n",
    "        cdf.name = label\n",
    "        series.append(pd.Series(cdf.index, name=label + ' - x'))\n",
    "        series.append(pd.Series(cdf['sum'].values, name=label + ' - y'))\n",
    "    \n",
    "    df = pd.concat(series, axis=1)\n",
    "    df.to_csv('../csvs/linreg-%s-cdf.csv' % field.replace('_', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_ip_hops_mean', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('IP hops mean')\n",
    "plt.savefig('../figures/linreg-slope-ip-hops-mean-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_as_hops_mean', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('AS hops mean')\n",
    "plt.savefig('../figures/linreg-slope-as-hops-mean-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_ip_hops_median', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('IP hops median')\n",
    "plt.savefig('../figures/linreg-slope-ip-hops-median-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_as_hops_median', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('AS hops median')\n",
    "plt.savefig('../figures/linreg-slope-as-hops-median-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.groupby(['type', 'ixp_hops']).agg({'slope_ip_hops_mean': 'describe', 'slope_as_hops_mean': 'describe', 'slope_ip_hops_median': 'describe', 'slope_as_hops_median': 'describe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot['dst_asn_rank'] = df_plot['dst_asn'].apply(lambda asn: as_rank_dict.get(asn, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot['asccreach'] = df_plot['dst_asn'].apply(lambda asn: asccreach[datetime.datetime(2016, 8, 1, 0, 0)].get(asn, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot['ascc'] = df_plot['dst_asn'].apply(lambda asn: len(customercones[datetime.datetime(2016, 8, 1, 0, 0)].get(asn, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.to_csv('../csvs/ascc-vs-slope-ip-vs-slope-as.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=figsize_half, sharey=True, sharex=True)\n",
    "\n",
    "df_plot.plot.scatter('slope_ip_hops_median', 'ascc', ax=ax[0])\n",
    "df_plot.plot.scatter('slope_as_hops_median', 'ascc', ax=ax[1])\n",
    "\n",
    "plt.savefig('../figures/ascc-vs-slope-ip-vs-slope-as.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=figsize_half, sharey=True, sharex=True)\n",
    "\n",
    "df_plot.plot.scatter('slope_ip_hops_median', 'n_traces', ax=ax[0])\n",
    "df_plot.plot.scatter('slope_as_hops_median', 'n_traces', ax=ax[1])\n",
    "\n",
    "#plt.savefig('../figures/ascc-vs-slope-ip-vs-slope-as.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_counts = df.groupby(['type', 'ixp_hops', 'dst_asn']).agg({'n_traces': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_plot.merge(tr_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot['total_change_ip_hops_mean'] = df_plot['n_traces'] * df_plot['slope_ip_hops_mean']\n",
    "df_plot['total_change_as_hops_mean'] = df_plot['n_traces'] * df_plot['slope_as_hops_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "do_plot('total_change_as_hops_mean', ax)\n",
    "ax.set_xlim([-10000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plot(field, ax):\n",
    "    for key, group in df_plot.groupby(['type', 'ixp_hops']):\n",
    "        group = group.sort_values(field)\n",
    "        group['sum'] = 1.0 / len(group)\n",
    "        cdf = group.groupby(field).agg({'sum': 'sum'})\n",
    "        cdf['sum'] = cdf['sum'].cumsum()\n",
    "\n",
    "        label = {'caida-ark': 'Ark', 'iplane': 'iPlane'}[key[0]]\n",
    "        label += ' - '\n",
    "        label += {0: 'No IXP', 1: 'IXP'}[key[1]]\n",
    "        ax.plot(cdf, label=label)\n",
    "        ax.set_xlabel('Rate of change (hops/year)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuck = {asn: get_number_IPs_from_prefixes(pfxes) for asn, pfxes in asn2pfx[datetime.datetime(2016, 8, 1, 16, 0)].iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_plot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['fuck_value'] = df_test['dst_asn'].apply(lambda asn: fuck.get(asn, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.plot.scatter('fuck_value', 'ascc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 ASes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db['y_combined_type_date_ixp_hops'].find({}, {'_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=figsize_full, sharex=True, sharey=True)\n",
    "ax = list(itertools.chain(*ax))\n",
    "\n",
    "gb = df.set_index('date').groupby(['type', 'ixp_hops'])\n",
    "field = 'number_t1_ases_mean'\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'iplane', 0)).plot(ax=_ax, marker='o', y=field)\n",
    "_ax.set_title('iPlane, no-IXP');\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'iplane', 1)).plot(ax=_ax, marker='o', y=field)\n",
    "_ax.set_title('iPlane, IXP');\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'caida-ark', 0)).plot(ax=_ax, marker='o', y=field)\n",
    "_ax.set_title('Ark, no-IXP');\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'caida-ark', 1)).plot(ax=_ax, marker='o', y=field)\n",
    "_ax.set_title('Ark, IXP');\n",
    "\n",
    "plt.savefig('../figures/number_t1_ases_mean_grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=figsize_full, sharex=True, sharey=True)\n",
    "ax = list(itertools.chain(*ax))\n",
    "\n",
    "gb = df.set_index('date').groupby(['type', 'ixp_hops'])\n",
    "field = ['number_t1_ases_q1', 'number_t1_ases_q3']\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'iplane', 0))[field].plot(ax=_ax, marker='o')\n",
    "_ax.set_title('iPlane, no-IXP');\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'iplane', 1))[field].plot(ax=_ax, marker='o')\n",
    "_ax.set_title('iPlane, IXP');\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'caida-ark', 0))[field].plot(ax=_ax, marker='o')\n",
    "_ax.set_title('Ark, no-IXP');\n",
    "\n",
    "_ax = ax.pop(0)\n",
    "gb.get_group((u'caida-ark', 1))[field].plot(ax=_ax, marker='o')\n",
    "_ax.set_title('Ark, IXP');\n",
    "\n",
    "plt.savefig('../figures/number_t1_ases_q1_q3_grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "gb = df.set_index('date').groupby(['type', 'ixp_hops'])\n",
    "field = 'number_t1_ases_mean'\n",
    "\n",
    "ax.plot(gb.get_group(('iplane', 0))[field], marker='o', label='iPlane - no IXP')\n",
    "ax.plot(gb.get_group(('iplane', 1))[field], marker='o', label='iPlane - IXP')\n",
    "ax.plot(gb.get_group(('caida-ark', 0))[field], marker='o', label='Ark - no IXP')\n",
    "ax.plot(gb.get_group(('caida-ark', 1))[field], marker='o', label='Ark - IXP')\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('Average number of T1 AS on paths')\n",
    "\n",
    "plt.savefig('../figures/mean-number-t1-ases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.pivot_table(df, index='date', columns=['type', 'ixp_hops'], values='number_t1_ases_mean')\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp.to_csv('../csvs/number_t1_ases_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_data():\n",
    "    for entry in db['link_types'].find({'link_counts': {'$ne': 'total'}}, {'_id': 0}):\n",
    "        link_counts = { 'link_%s' % k: v for k, v in dict(entry['link_counts']).iteritems() }\n",
    "        entry.update(link_counts)\n",
    "        del(entry['link_counts'])\n",
    "        yield entry\n",
    "\n",
    "df = pd.DataFrame.from_records(get_data())\n",
    "df = df.fillna(0)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_hops = (df['link_*'] + df['link_?'] + df['link_cp'] + df['link_p'] + df['link_pc'])\n",
    "total_hops = (df['link_cp'] + df['link_p'] + df['link_pc'])\n",
    "df['link_cp_norm'] = df['link_cp'] / total_hops\n",
    "df['link_p_norm'] = df['link_p'] / total_hops\n",
    "df['link_pc_norm'] = df['link_pc'] / total_hops\n",
    "df = df.dropna(subset=['link_cp_norm', 'link_p_norm', 'link_pc_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average = lambda x: np.average(x.values, weights=df.loc[x.index, 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'type', 'number_ixp_hops']).agg({'link_p_norm': weighted_average, 'link_cp_norm': weighted_average, 'link_pc_norm': weighted_average})\n",
    "gb = gb.reset_index()\n",
    "\n",
    "gb['link_transit_norm'] = gb['link_pc_norm'] + gb['link_cp_norm']\n",
    "\n",
    "display(gb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gb['link_p_norm'] + gb['link_pc_norm'] + gb['link_cp_norm']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "g = gb.groupby(['type', 'number_ixp_hops'])\n",
    "\n",
    "g.get_group(('iplane', 0)).plot.scatter('link_p_norm', 'link_transit_norm', ax=ax, color='red', label='iPlane - no IXP')\n",
    "g.get_group(('iplane', 1)).plot.scatter('link_p_norm', 'link_transit_norm', ax=ax, color='green', label='iPlane - IXP')\n",
    "\n",
    "g.get_group(('caida-ark', 0)).plot.scatter('link_p_norm', 'link_transit_norm', ax=ax, color='pink', label='Caida - no IXP')\n",
    "g.get_group(('caida-ark', 1)).plot.scatter('link_p_norm', 'link_transit_norm', ax=ax, color='turquoise', label='Caida - IXP')\n",
    "\n",
    "plt.savefig('../figures/link-types-scatter-plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'number_ixp_hops']).agg({'link_p_norm': weighted_average, 'link_cp_norm': weighted_average, 'link_pc_norm': weighted_average})\n",
    "gb = gb.reset_index()\n",
    "\n",
    "gb['link_transit_norm'] = gb['link_pc_norm'] + gb['link_cp_norm']\n",
    "\n",
    "display(gb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = gb.copy()\n",
    "exp = exp.pivot(index='date', columns='number_ixp_hops')\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp.head()\n",
    "exp.to_csv('../csvs/link-types-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=figsize_half, sharey=True)\n",
    "\n",
    "flierprops = dict(marker='.', markerfacecolor='black', markersize=12, linestyle='none')\n",
    "\n",
    "g = gb.groupby('number_ixp_hops')\n",
    "\n",
    "ax[0].boxplot([g.get_group(0)['link_p_norm'], g.get_group(0)['link_cp_norm'], g.get_group(0)['link_pc_norm']], labels=['p', 'cp', 'pc'], whis='range');\n",
    "ax[0].set_title('No-IXP');\n",
    "ax[0].set_ylabel('Relative Share')\n",
    "\n",
    "ax[1].boxplot([g.get_group(1)['link_p_norm'], g.get_group(1)['link_cp_norm'], g.get_group(1)['link_pc_norm']], labels=['p', 'cp', 'pc'], whis='range');\n",
    "ax[1].set_title('IXP');\n",
    "\n",
    "plt.savefig('../figures/link-types-boxplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    pd.Series(g.get_group(0)['link_p_norm'].values),\n",
    "    pd.Series(g.get_group(0)['link_cp_norm'].values),\n",
    "    pd.Series(g.get_group(0)['link_pc_norm'].values),\n",
    "    pd.Series(g.get_group(1)['link_p_norm'].values),\n",
    "    pd.Series(g.get_group(1)['link_cp_norm'].values),\n",
    "    pd.Series(g.get_group(1)['link_pc_norm'].values),\n",
    "], axis='columns', keys=['p - no IXP', 'cp - no IXP', 'pc - no IXP', 'p - IXP', 'cp - IXP', 'pc - IXP']).to_csv('../csvs/link-types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=figsize_half, sharey=True)\n",
    "\n",
    "flierprops = dict(marker='.', markerfacecolor='black', markersize=12, linestyle='none')\n",
    "\n",
    "g = gb.groupby('number_ixp_hops')\n",
    "\n",
    "ax[0].boxplot([g.get_group(0)['link_p_norm'], g.get_group(1)['link_p_norm']], labels=['no-IXP', 'IXP'], whis=[1,99]);\n",
    "ax[0].set_title('link_p_norm');\n",
    "\n",
    "ax[1].boxplot([g.get_group(0)['link_cp_norm'], g.get_group(1)['link_cp_norm']], labels=['no-IXP', 'IXP'], whis=[1,99]);\n",
    "ax[1].set_title('link_cp_norm');\n",
    "\n",
    "ax[2].boxplot([g.get_group(0)['link_pc_norm'], g.get_group(1)['link_pc_norm']], labels=['no-IXP', 'IXP'], whis=[1,99]);\n",
    "ax[2].set_title('link_pc_norm');\n",
    "\n",
    "#plt.savefig('../figures/link-types-boxplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "with bz2.BZ2File('../code/traceroutes/pickles/customercones_asccreach.picklev2.bz2') as f:\n",
    "    customercones, asccreach = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = db['centrality'].distinct('date')\n",
    "date_mapping = {pd.to_datetime(date): min(asccreach.keys(), key=lambda x: abs(pd.to_datetime(x)-date)) for date in dates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_mapping2 = {pd.to_datetime(date): min(t1_ases.keys(), key=lambda x: abs(pd.to_datetime(x)-date)) for date in dates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records():\n",
    "    for entry in tqdm.tqdm_notebook(\n",
    "        db['centrality'].find({'$and': [{'asn': {'$ne': -1}}, {'asn': {'$ne': 'total'}}]}, {'_id': 0}),\n",
    "        leave=False\n",
    "    ):\n",
    "        entry['asccreach'] = asccreach[date_mapping[entry['date']]].get(entry['asn'], 0)\n",
    "        entry['asccsize'] = len(customercones[date_mapping[entry['date']]].get(entry['asn'], []))\n",
    "        entry['as_rank'] = as_rank_dict.get(entry['asn'], -1)\n",
    "        entry['t1'] = entry['asn'] in t1_ases[date_mapping2[entry['date']]]\n",
    "        yield entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame.from_records(get_records())\n",
    "\n",
    "df_total = pd.DataFrame.from_records(db['centrality'].find({'asn': 'total'}, {'_id': 0, 'asn': 0}))\n",
    "df_total = df_total.rename(columns={'count': 'total_count'})\n",
    "df = df.merge(df_total, on=['date', 'type', 'number_ixp_hops'])\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['centrality_percent'] = 100 * df['count'] / df['total_count']\n",
    "df = df.join(df.groupby(['date', 'number_ixp_hops', 'type'])['centrality_percent'].rank(method='min', ascending=False).to_frame(name='rank'))\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['number_ixp_hops', 'asn']).agg({'centrality_percent': 'mean'})\n",
    "df_plot = gb.reset_index()\n",
    "display(df_plot.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "for key, group in df_plot.groupby(['number_ixp_hops']):\n",
    "    ax.plot(range(1, 26), group.nlargest(25, 'centrality_percent')['centrality_percent'], label={0: 'No-IXP', 1: 'IXP'}[key], marker='o')\n",
    "    \n",
    "ax.legend(frameon=True)\n",
    "ax.set_ylabel('Avg. centrality (%)')\n",
    "ax.set_xlabel('Networks sorted by centrality')\n",
    "plt.savefig('../figures/centrality-mean-all-snapshots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = df_plot[df_plot['number_ixp_hops'] == 0].sort_values('centrality_percent', ascending=False)['centrality_percent'].reset_index(drop=True)\n",
    "s0.name = 'No-IXP'\n",
    "s1 = df_plot[df_plot['number_ixp_hops'] == 1].sort_values('centrality_percent', ascending=False)['centrality_percent'].reset_index(drop=True)\n",
    "s1.name = 'IXP'\n",
    "pd.concat([s0, s1], axis='columns').to_csv('../csvs/centrality-mean-all-snapshots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', 4):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "    gb = df.groupby(['date', 'type', 'number_ixp_hops'])\n",
    "    gb = gb.apply(lambda g: g.nlargest(10, 'centrality_percent')['as_rank'].mean())\n",
    "    gb = gb.unstack('type').unstack('number_ixp_hops')\n",
    "    gb.plot(ax=ax)\n",
    "\n",
    "    ax.set_ylabel('Avg. AS Rank of Top 10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', 4):\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=figsize_full, sharey=False, sharex=True)\n",
    "    ax = list(itertools.chain(*ax))\n",
    "    \n",
    "    for (ax, size) in zip(ax, [10, 50, 250, 500]):\n",
    "        \n",
    "        ax.set_title('Top %s' % size)\n",
    "        \n",
    "        gb = df.groupby(['date', 'type', 'number_ixp_hops'])\n",
    "        gb = gb.apply(lambda g: g.nlargest(size, 'centrality_percent')['asccsize'].mean())\n",
    "        gb = gb.unstack('type').unstack('number_ixp_hops')\n",
    "        gb.plot(ax=ax, legend=False)\n",
    "        \n",
    "        gb.columns = ['/'.join(map(str,c)) for c in gb.columns]\n",
    "        gb.to_csv('../csvs/asccsize-grid-top-%s.csv' % size)\n",
    "        \n",
    "    plt.legend(*ax.get_legend_handles_labels(), loc='lower center', frameon=True, ncol=4, bbox_to_anchor=(0, 2.35, -0.25, 1.25))\n",
    "    plt.savefig('../figures/asccsize-grid', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['date', 'type', 'number_ixp_hops', 't1']).agg({'rank': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group['group_rank'] = group['centrality_percent'].rank(method='min', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.color_palette('hls', 4):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "    gb = df.groupby(['date', 'type', 'number_ixp_hops'])\n",
    "    gb = gb.apply(lambda g: g.nlargest(25, 'centrality_percent')['asccreach'].sum())\n",
    "    gb = gb.unstack('type').unstack('number_ixp_hops')\n",
    "    gb.plot(ax=ax)\n",
    "\n",
    "    ax.set_ylabel('Avg. AS Rank of Top 10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'number_ixp_hops', 'type']).agg({'centrality_percent': ['mean',\n",
    "                                                                                  'median', \n",
    "                                                                                  percentile(0.5), \n",
    "                                                                                  percentile(5), \n",
    "                                                                                  percentile(25), \n",
    "                                                                                  percentile(75), \n",
    "                                                                                  percentile(95), \n",
    "                                                                                  percentile(99), \n",
    "                                                                                  percentile(99.5), \n",
    "                                                                                  'max', \n",
    "                                                                                  'min']}).unstack().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.plot(gb[('centrality_percent', 'max', 'iplane', 0)], label='iPlane - no IXP')\n",
    "ax.plot(gb[('centrality_percent', 'max', 'iplane', 1)], label='iPlane - IXP')\n",
    "ax.plot(gb[('centrality_percent', 'max', 'caida-ark', 0)], label='Caida Ark - no IXP')\n",
    "ax.plot(gb[('centrality_percent', 'max', 'caida-ark', 1)], label='Caida Ark - IXP')\n",
    "\n",
    "plt.legend(frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.plot(gb[('centrality_percent', 'mean', 'iplane', 0)], label='iPlane - no IXP')\n",
    "ax.plot(gb[('centrality_percent', 'mean', 'iplane', 1)], label='iPlane - IXP')\n",
    "ax.plot(gb[('centrality_percent', 'mean', 'caida-ark', 0)], label='Caida Ark - no IXP')\n",
    "ax.plot(gb[('centrality_percent', 'mean', 'caida-ark', 1)], label='Caida Ark - IXP')\n",
    "\n",
    "plt.legend(frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "\n",
    "ax.plot(gb[('centrality_percent', 'median', 'iplane', 0)], label='iPlane - no IXP')\n",
    "ax.fill_between(gb.index, gb[('centrality_percent', 'percentile_25', 'iplane', 0)], gb[('centrality_percent', 'percentile_75', 'iplane', 0)], alpha=0.25)\n",
    "#ax.fill_between(gb.index, gb[('centrality_percent', 'percentile_5', 'iplane', 0)], gb[('centrality_percent', 'percentile_95', 'iplane', 0)], alpha=0.25)\n",
    "\n",
    "ax.plot(gb[('centrality_percent', 'median', 'iplane', 1)], label='iPlane - IXP')\n",
    "ax.fill_between(gb.index, gb[('centrality_percent', 'percentile_25', 'iplane', 1)], gb[('centrality_percent', 'percentile_75', 'iplane', 1)], alpha=0.25)\n",
    "#ax.fill_between(gb.index, gb[('centrality_percent', 'min', 'iplane', 1)], gb[('centrality_percent', 'max', 'iplane', 1)], alpha=0.25)\n",
    "\n",
    "plt.legend(frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "\n",
    "ax.plot(gb[('centrality_percent', 'median', 'caida-ark', 0)], label='caida-ark - no IXP')\n",
    "#ax.plot(gb[('centrality_percent', 'max', 'caida-ark', 0)], label='caida-ark - no IXP')\n",
    "ax.fill_between(gb.index, gb[('centrality_percent', 'min', 'caida-ark', 0)], gb[('centrality_percent', 'percentile_75', 'caida-ark', 0)], alpha=0.25)\n",
    "\n",
    "ax.plot(gb[('centrality_percent', 'median', 'caida-ark', 1)], label='caida-ark - IXP')\n",
    "#ax.plot(gb[('centrality_percent', 'max', 'caida-ark', 1)], label='caida-ark - IXP')\n",
    "ax.fill_between(gb.index, gb[('centrality_percent', 'min', 'caida-ark', 1)], gb[('centrality_percent', 'percentile_75', 'caida-ark', 1)], alpha=0.25)\n",
    "\n",
    "plt.legend(frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(2, figsize=figsize_full)\n",
    "\n",
    "ax[0].fill_between(gb.index, gb[('centrality_percent', 'percentile_99.5', 'iplane', 0)], gb[('centrality_percent', 'max', 'iplane', 0)], alpha=0.25, label='iPlane - no IXP')\n",
    "ax[0].fill_between(gb.index, gb[('centrality_percent', 'percentile_99.5', 'iplane', 1)], gb[('centrality_percent', 'max', 'iplane', 1)], alpha=0.25, label='iPlane - IXP')\n",
    "ax[0].legend(frameon=True)\n",
    "\n",
    "ax[1].fill_between(gb.index, gb[('centrality_percent', 'percentile_99.5', 'caida-ark', 0)], gb[('centrality_percent', 'max', 'caida-ark', 0)], alpha=0.25, label='caida-ark - no IXP')\n",
    "ax[1].fill_between(gb.index, gb[('centrality_percent', 'percentile_99.5', 'caida-ark', 1)], gb[('centrality_percent', 'max', 'caida-ark', 1)], alpha=0.25, label='caida-ark - IXP')\n",
    "ax[1].legend(frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ASes in Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = []\n",
    "for t in ['iplane', 'caida-ark']:\n",
    "    for n in [0, 1]:\n",
    "        seen_asn = set()\n",
    "        for ts in tqdm.tqdm_notebook(sorted(df[ (df['type'] == t) & (df['number_ixp_hops'] == n) ]['date'].unique())):\n",
    "            s = df[ (df['type'] == t) & (df['number_ixp_hops'] == n) & (df['date'] == ts) ].nlargest(10, 'centrality_percent')['asn'].values\n",
    "            seen_asn.update(s)\n",
    "            res.append({\n",
    "                'date': ts,\n",
    "                'type': t,\n",
    "                'number_ixp_hops': n,\n",
    "                'set_size': len(seen_asn)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "pd.pivot_table(pd.DataFrame(res), index='date', columns=['type', 'number_ixp_hops'], values='set_size').plot(ax=ax)\n",
    "ax.set_ylabel('# of ASes in Top 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative List Change Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_change_factor(l1, l2):\n",
    "    unique_elements = set(l1 + l2)\n",
    "    change_sum = 0\n",
    "    for x in unique_elements:\n",
    "        \n",
    "        try:\n",
    "            index1 = l1.index(x)\n",
    "        except ValueError:\n",
    "            index1 = len(l1)\n",
    "            \n",
    "        try:\n",
    "            index2 = l2.index(x)\n",
    "        except ValueError:\n",
    "            index2 = len(l1)\n",
    "            \n",
    "        change_sum += abs(index2 - index1)\n",
    "    return change_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['type', 'number_ixp_hops', 'date'])\n",
    "gb = gb.apply(lambda g: g.nlargest(10, 'centrality_percent')['asn'].values)\n",
    "display(gb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_change_factor(list(gb.loc['caida-ark', 1, '2007-10-01']), list(gb.loc['caida-ark', 1, '2008-01-01']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.intersection( *(set(l) for l in gb.loc['iplane', 0].values) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peerings at IXPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db['resilience'].find({}, {'_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tmp = defaultdict(lambda: set())\n",
    "\n",
    "for record in db['resilience'].find({}, {'_id': 0}):\n",
    "    if record['as1'] == '*':\n",
    "        continue\n",
    "    if record['as2'] == '*':\n",
    "        continue\n",
    "        \n",
    "    res_tmp[record['date']].add((int(record['as1']), int(record['as2'])))\n",
    "    \n",
    "res_tmp = dict(res_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_traceroute_peering_link_coverage():\n",
    "    for date in tqdm.tqdm_notebook(res_tmp.keys(), leave=False):\n",
    "        nearest_date = min(as_relations.iterkeys(), key=lambda x: abs(date-x))\n",
    "        peering_links = [ (as1, as2) for (as1, as2), rel in as_relations[nearest_date].iteritems() if rel == 0 ]\n",
    "        assert len(peering_links) == len(set([ (as1, as2) if as1 <= as2 else (as2, as1) for (as1, as2) in peering_links ]))\n",
    "\n",
    "        total_peerings = len(peering_links)\n",
    "        \n",
    "        in_ixp_peerings = 0\n",
    "        out_ixp_peerings = 0\n",
    "        for (as1, as2) in peering_links:\n",
    "\n",
    "            if (as1, as2) in res_tmp[date]:\n",
    "                in_ixp_peerings += 1\n",
    "            elif (as2, as1) in res_tmp[date]:\n",
    "                in_ixp_peerings += 1\n",
    "            else:\n",
    "                out_ixp_peerings += 1\n",
    "\n",
    "        yield(dict(date=date, in_ixp_peerings=in_ixp_peerings, out_ixp_peerings=out_ixp_peerings, total_peerings=total_peerings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(calc_traceroute_peering_link_coverage())\n",
    "df = df.set_index('date')\n",
    "df['ratio'] = df['in_ixp_peerings'] / df['total_peerings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "df.plot(ax=ax, marker='o')\n",
    "\n",
    "plt.savefig('../figures/traceroute-in-ixp-vs-out-ixp-peerings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "df['ratio'].plot(ax=ax, marker='o')\n",
    "\n",
    "plt.savefig('../figures/traceroute-in-ixp-vs-out-ixp-peerings-ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixp_member_asn_set = {\n",
    "    date: {\n",
    "        ixp: set(member_asn) for ixp, member_asn in bla.iteritems()\n",
    "    } for date, bla in ixp_member_asn.iteritems()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ixp_peering(date, as1, as2):\n",
    "    for ixp, member_asn in ixp_member_asn_set[date].iteritems():\n",
    "        if as1 in member_asn and as2 in member_asn:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pdb_peering_link_coverage():\n",
    "    for date in tqdm.tqdm_notebook(ixp_member_asn_set.keys(), leave=False):\n",
    "        nearest_date = min(as_relations.iterkeys(), key=lambda x: abs(date-x))\n",
    "        peering_links = [ (as1, as2) for (as1, as2), rel in as_relations[nearest_date].iteritems() if rel == 0 ]\n",
    "        assert len(peering_links) == len(set([ (as1, as2) if as1 <= as2 else (as2, as1) for (as1, as2) in peering_links ]))\n",
    "\n",
    "        total_peerings = len(peering_links)\n",
    "\n",
    "        in_ixp_peerings = 0\n",
    "        out_ixp_peerings = 0\n",
    "        for (as1, as2) in peering_links:\n",
    "\n",
    "            if is_ixp_peering(date, as1, as2):\n",
    "                in_ixp_peerings += 1\n",
    "            else:\n",
    "                out_ixp_peerings += 1\n",
    "\n",
    "        yield(dict(date=date, in_ixp_peerings=in_ixp_peerings, out_ixp_peerings=out_ixp_peerings, total_peerings=total_peerings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(calc_pdb_peering_link_coverage())\n",
    "df = df.set_index('date')\n",
    "df['ratio'] = df['in_ixp_peerings'] / df['total_peerings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "\n",
    "df.plot(ax=ax, marker='o')\n",
    "\n",
    "plt.savefig('../figures/peeringdb-in-ixp-vs-out-ixp-peerings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().sort_values('date').to_csv('../csvs/peeringdb-in-ixp-vs-out-ixp-peerings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "\n",
    "df['ratio'].plot(ax=ax, marker='o')\n",
    "\n",
    "plt.savefig('../figures/peeringdb-in-ixp-vs-out-ixp-peerings-ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colocation_multiplicity(date, as1, as2):\n",
    "    count = 0\n",
    "    for ixp, member_asn in ixp_member_asn_set[date].iteritems():\n",
    "        if as1 in member_asn and as2 in member_asn:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pdb_colocation_multiplicity():\n",
    "    for date in tqdm.tqdm_notebook(ixp_member_asn_set.keys(), leave=False):\n",
    "        nearest_date = min(as_relations.iterkeys(), key=lambda x: abs(date-x))\n",
    "        peering_links = [ (as1, as2) for (as1, as2), rel in as_relations[nearest_date].iteritems() if rel == 0 ]\n",
    "        assert len(peering_links) == len(set([ (as1, as2) if as1 <= as2 else (as2, as1) for (as1, as2) in peering_links ]))\n",
    "        \n",
    "        for (as1, as2) in peering_links:\n",
    "            yield(dict(date=date, as1=as1, as2=as2, colocation_multiplicity=colocation_multiplicity(date, as1, as2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(calc_pdb_colocation_multiplicity())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_full)\n",
    "    \n",
    "gb = df.groupby('date')\n",
    "\n",
    "v = [gb.get_group(d)['colocation_multiplicity'] for d in sorted(df['date'].unique())]\n",
    "l = [pd.to_datetime(d).strftime('%Y') for d in sorted(df['date'].unique())]\n",
    "\n",
    "ax.boxplot(v, labels=l, whis='range');\n",
    "\n",
    "plt.savefig('../figures/colocation-multiplicity-boxplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Subnet path length changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame.from_records(db['subnet_linreg_slope'].find(projection={'_id': 0}))\n",
    "df['date_diff'] = df['date_max'] - df['date_min']\n",
    "print(len(df))\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(df_plot['date_diff'].astype(int) / 10**9 / (3600*24*360), df_plot['slope_as_hops_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df[df['date_diff'] >= '720 days'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(df_plot['date_diff'].astype(int) / 10**9 / (3600*24*360), df_plot['slope_as_hops_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plot(field, ax):\n",
    "    series = []\n",
    "    for key, group in df_plot.groupby(['type', 'ixp_hops']):\n",
    "        group = group.sort_values(field)\n",
    "        group['sum'] = 1.0 / len(group)\n",
    "        cdf = group.groupby(field).agg({'sum': 'sum'})\n",
    "        cdf['sum'] = cdf['sum'].cumsum()\n",
    "\n",
    "        label = {'caida-ark': 'Ark', 'iplane': 'iPlane'}[key[0]]\n",
    "        label += ' - '\n",
    "        label += {0: 'No IXP', 1: 'IXP'}[key[1]]\n",
    "        ax.plot(cdf, label=label)\n",
    "        ax.set_xlabel('Rate of change (hops/year)')\n",
    "        \n",
    "        cdf.name = label\n",
    "        series.append(pd.Series(cdf.index, name=label + ' - x'))\n",
    "        series.append(pd.Series(cdf['sum'].values, name=label + ' - y'))\n",
    "    \n",
    "    df = pd.concat(series, axis=1)\n",
    "    df.to_csv('../csvs/linreg-per-subnet-%s-cdf.csv.bz2' % field.replace('_', '-'), compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_ip_hops_mean', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('IP hops mean');\n",
    "plt.savefig('../figures/linreg-slope-subnet-ip-hops-mean-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_as_hops_mean', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('AS hops mean');\n",
    "plt.savefig('../figures/linreg-slope-subnet-as-hops-mean-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_ip_hops_median', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('IP hops median');\n",
    "plt.savefig('../figures/linreg-slope-subnet-ip-hops-median-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_as_hops_median', ax)\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title('AS hops median');\n",
    "plt.savefig('../figures/linreg-slope-subnet-as-hops-median-cdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subnet Coverage Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pfx_coverage(params):\n",
    "    date, t = params\n",
    "    \n",
    "    client = pymongo.MongoClient('mongodb://ixp_history:ixp_history@localhost:27017/ixp_history')\n",
    "    db = client['ixp_history']\n",
    "    collection = db['y_combined_type_date_ixp_hops_dst_subnet24']\n",
    "    nearest_date = min(asn2pfx.iterkeys(), key=lambda x: abs(x-date))\n",
    "    \n",
    "    announced_prefixes = list(itertools.chain(*[pfxes for pfxes in asn2pfx[nearest_date].itervalues()]))\n",
    "    pyt = pytricia.PyTricia()\n",
    "    pyt24 = pytricia.PyTricia()\n",
    "    for pfx in announced_prefixes:\n",
    "        pyt[pfx] = pfx\n",
    "        subnet, submask = pfx.split('/')\n",
    "        submask = int(submask)\n",
    "        if submask <= 24:\n",
    "            pyt24[pfx] = pfx\n",
    "        \n",
    "    covered_pfx = set()\n",
    "    covered_pfx_parents = set()\n",
    "    \n",
    "    covered_pfx_24 = set()\n",
    "    covered_pfx_parents_24 = set()\n",
    "    if t != 'total':\n",
    "        target_subnets = set((str(entry['subnet'] + '.1') for entry in collection.find({'date': date, 'type': t}, {'subnet': 1})))\n",
    "    else:\n",
    "        target_subnets = set((str(entry['subnet'] + '.1') for entry in collection.find({'date': date}, {'subnet': 1})))\n",
    "    for ipaddr in target_subnets:\n",
    "        try:\n",
    "            pfx = pyt[ipaddr]\n",
    "            covered_pfx.add(pfx)\n",
    "            covered_pfx_parents.add(pfx)\n",
    "            while pyt.parent(pfx) != None:\n",
    "                pfx = pyt.parent(pfx)\n",
    "                covered_pfx_parents.add(pfx)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            pfx = pyt24[ipaddr]\n",
    "            covered_pfx_24.add(pfx)\n",
    "            covered_pfx_parents_24.add(pfx)\n",
    "            while pyt24.parent(pfx) != None:\n",
    "                pfx = pyt24.parent(pfx)\n",
    "                covered_pfx_parents_24.add(pfx)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    client.close()\n",
    "    return (dict(\n",
    "        date=date,\n",
    "        type=t,\n",
    "        announced_prefixes=len(announced_prefixes),\n",
    "        covered_pfx=len(covered_pfx),\n",
    "        covered_pfx_parents=len(covered_pfx_parents),\n",
    "        announced_prefixes_24=len(pyt24),\n",
    "        covered_pfx_24=len(covered_pfx_24),\n",
    "        covered_pfx_parents_24=len(covered_pfx_parents_24),\n",
    "        \n",
    "        announced_prefixes_size=get_number_IPs_from_pyt(pyt),\n",
    "        covered_pfx_size=get_number_IPs_from_prefixes(covered_pfx),\n",
    "        covered_pfx_parents_size=get_number_IPs_from_prefixes(covered_pfx_parents),\n",
    "        announced_prefixes_24_size=get_number_IPs_from_pyt(pyt24),\n",
    "        covered_pfx_24_size=get_number_IPs_from_prefixes(covered_pfx_24),\n",
    "        covered_pfx_parents_24_size=get_number_IPs_from_prefixes(covered_pfx_parents_24),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['y_combined_type_date_ixp_hops_dst_subnet24']\n",
    "tasks = [(date, t) for date in collection.distinct('date') for t in collection.distinct('type', query={'date': date})]\n",
    "tasks += [(date, 'total') for date in collection.distinct('date')]\n",
    "\n",
    "pool =  multiprocessing.Pool(8)\n",
    "prefix_counts = list(tqdm.tqdm_notebook(pool.imap(calc_pfx_coverage, tasks), total=len(tasks)))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prefix_counts)\n",
    "df['ratio_covered'] = df['covered_pfx'] / df['announced_prefixes']\n",
    "df['ratio_covered_parents'] = df['covered_pfx_parents'] / df['announced_prefixes']\n",
    "df['ratio_covered_24'] = df['covered_pfx_24'] / df['announced_prefixes_24']\n",
    "df['ratio_covered_parents_24'] = df['covered_pfx_parents_24'] / df['announced_prefixes_24']\n",
    "\n",
    "df['ratio_size_covered'] = df['covered_pfx_size'] / df['announced_prefixes_size']\n",
    "df['ratio_size_covered_parents'] = df['covered_pfx_parents_size'] / df['announced_prefixes_size']\n",
    "df['ratio_size_covered_24'] = df['covered_pfx_24_size'] / df['announced_prefixes_24_size']\n",
    "df['ratio_size_covered_parents_24'] = df['covered_pfx_parents_24_size'] / df['announced_prefixes_24_size']\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df.pivot(index='date', columns='type')\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp.to_csv('../csvs/pfx_covered.csv')\n",
    "display(exp.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "df.pivot_table(index='date', columns='type', values=['ratio_covered_24', 'ratio_covered_parents_24']).plot(ax=ax)\n",
    "plt.savefig('../figures/pfx_covered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "df.pivot_table(index='date', columns='type', values=['ratio_size_covered_24', 'ratio_size_covered_parents_24']).plot(ax=ax)\n",
    "plt.savefig('../figures/pfx_covered_size_ip_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df.pivot_table(index='date', columns='type', values=['ratio_covered', 'ratio_covered_parents', 'ratio_covered_24', 'ratio_covered_parents_24'])\n",
    "df_t = df_t.describe().transpose().unstack('type')\n",
    "df_t = df_t.drop(labels=['count', '25%', '50%', '75%'], axis='columns')\n",
    "df_t = df_t.reorder_levels(order=[1, 0], axis='columns')\n",
    "df_t = df_t.sort_index(axis='columns')\n",
    "df_t = df_t.transpose()\n",
    "print(df_t.to_latex(float_format='%.3f', multirow=True))\n",
    "display(df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak-valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db['y_combined_peak_valley'].find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "df.pivot_table(index='date', columns=['type', 'number_ixp_hops'], values='cc_ratio_mean').plot(ax=ax)\n",
    "\n",
    "plt.savefig('../figures/peak-valley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df.pivot_table(index='date', columns=['type', 'number_ixp_hops'], values='cc_ratio_mean').copy()\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp.to_csv('../csvs/peak-valley.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per IXP slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ixp_slope = pd.DataFrame.from_records(db['y_combined_type_date_ixp_hops_plus_ixp'].find(projection={'_id': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ixp_slope.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linreg_slope(group):\n",
    "    \n",
    "    if len(group) < 2:\n",
    "        return None\n",
    "    \n",
    "    group = group.sort_values('date')\n",
    "    x = group['date'].values.astype(int) / 10**9 / 31104000.0\n",
    "    \n",
    "    y = group['as_hops_mean'].values\n",
    "    slope_as_hops_mean, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    y = group['ip_hops_mean'].values\n",
    "    slope_ip_hops_mean, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    y = group['as_hops_median'].values\n",
    "    slope_as_hops_median, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    y = group['ip_hops_median'].values\n",
    "    slope_ip_hops_median, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'date_min': group['date'].min(),\n",
    "        'date_max': group['date'].max(),\n",
    "        'slope_as_hops_mean': slope_as_hops_mean,\n",
    "        'slope_ip_hops_mean': slope_ip_hops_mean,\n",
    "        'slope_as_hops_median': slope_as_hops_median,\n",
    "        'slope_ip_hops_median': slope_ip_hops_median,\n",
    "    })\n",
    "\n",
    "gb = df_ixp_slope.groupby(['type', 'ixp']).apply(get_linreg_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = gb.reset_index()\n",
    "gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plot(field, ax):\n",
    "    series = []\n",
    "    for key, group in gb.groupby(['type']):\n",
    "        group = group.sort_values(field)\n",
    "        group['sum'] = 1.0 / len(group)\n",
    "        cdf = group.groupby(field).agg({'sum': 'sum'})\n",
    "        cdf['sum'] = cdf['sum'].cumsum()\n",
    "\n",
    "        label = {'caida-ark': 'Ark', 'iplane': 'iPlane'}[key]\n",
    "        ax.plot(cdf, label=label)\n",
    "        ax.set_xlabel('Rate of change (hops/year)')\n",
    "        \n",
    "        cdf.name = label\n",
    "        series.append(pd.Series(cdf.index, name=label + ' - x'))\n",
    "        series.append(pd.Series(cdf['sum'].values, name=label + ' - y'))\n",
    "    \n",
    "    df = pd.concat(series, axis=1)\n",
    "    df.to_csv('../csvs/linreg-per-ixp-%s-cdf.csv' % field.replace('_', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_ip_hops_mean', ax)\n",
    "plt.savefig('../figures/linreg-slope-per-ixp-ip-hops-mean-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_ip_hops_median', ax)\n",
    "plt.savefig('../figures/linreg-slope-per-ixp-ip-hops-median-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_as_hops_mean', ax)\n",
    "plt.savefig('../figures/linreg-slope-per-ixp-as-hops-mean-cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize_half)\n",
    "do_plot('slope_as_hops_median', ax)\n",
    "plt.savefig('../figures/linreg-slope-per-ixp-as-hops-median-cdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Validation - the next attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_ips = {ts: get_number_IPs_from_prefixes(list(itertools.chain(*[pfxes for _, pfxes in values.iteritems()]))) for ts, values in tqdm.tqdm_notebook(asn2pfx.items())}\n",
    "asn_count = {k: len(set(v.iterkeys())) for k, v in tqdm.tqdm_notebook(asn2pfx.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ips_from_asn_list(asn_list, date):\n",
    "    next_date = min(asn2pfx.keys(), key=lambda ts: abs(date-ts))\n",
    "    pfx = itertools.chain(*[asn2pfx[next_date].get(asn, []) for asn in asn_list])\n",
    "    return get_number_IPs_from_prefixes(pfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bias_pfx_coverage(data):\n",
    "    date = data['date']\n",
    "    nearest_date = min(asn2pfx.iterkeys(), key=lambda x: abs(x-date))\n",
    "    \n",
    "    announced_prefixes = list(itertools.chain(*[pfxes for pfxes in asn2pfx[nearest_date].itervalues()]))\n",
    "    # pyt = pytricia.PyTricia()\n",
    "    pyt24 = pytricia.PyTricia()\n",
    "    for pfx in announced_prefixes:\n",
    "        # pyt[pfx] = pfx\n",
    "        subnet, submask = pfx.split('/')\n",
    "        submask = int(submask)\n",
    "        if submask <= 24:\n",
    "            pyt24[pfx] = pfx\n",
    "        \n",
    "\n",
    "    \n",
    "    res = dict(\n",
    "        announced_prefixes_24=len(pyt24),\n",
    "        announced_prefixes_24_size=get_number_IPs_from_pyt(pyt24),\n",
    "    )\n",
    "    \n",
    "    for step in ['step0', 'step1', 'step2', 'step3', 'step4']:\n",
    "        covered_pfx = set()\n",
    "        covered_pfx_parents = set()\n",
    "\n",
    "        covered_pfx_24 = set()\n",
    "        covered_pfx_parents_24 = set()\n",
    "        \n",
    "        subnet_list = data['dst_ip_subnet24_' + step]\n",
    "        target_subnets = set((str(subnet + '.1') for subnet in subnet_list))\n",
    " \n",
    "        for ipaddr in target_subnets:\n",
    "            #try:\n",
    "            #   pfx = pyt[ipaddr]\n",
    "            #    covered_pfx.add(pfx)\n",
    "            #    covered_pfx_parents.add(pfx)\n",
    "            #    while pyt.parent(pfx) != None:\n",
    "            #        pfx = pyt.parent(pfx)\n",
    "            #        covered_pfx_parents.add(pfx)\n",
    "            #except KeyError:\n",
    "            #    pass\n",
    "\n",
    "            try:\n",
    "                pfx = pyt24[ipaddr]\n",
    "                covered_pfx_24.add(pfx)\n",
    "                covered_pfx_parents_24.add(pfx)\n",
    "                while pyt24.parent(pfx) != None:\n",
    "                    pfx = pyt24.parent(pfx)\n",
    "                    covered_pfx_parents_24.add(pfx)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        res['covered_pfx_24_' + step] = len(covered_pfx_24)\n",
    "        res['covered_pfx_parents_24_' + step] = len(covered_pfx_parents_24)\n",
    "        res['covered_pfx_24_size_' + step] = get_number_IPs_from_prefixes(covered_pfx_24)\n",
    "        res['covered_pfx_parents_24_size_' + step] = get_number_IPs_from_prefixes(covered_pfx_parents_24)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bias_data(f_json):\n",
    "    with bz2.BZ2File(f_json) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    with bz2.BZ2File(f_json.replace('bias_results', 'bias_results_step4')) as f2:\n",
    "        data_step4 = json.load(f2)\n",
    "        \n",
    "    assert data['date'] == data_step4['date']\n",
    "    assert data['type'] == data_step4['type']\n",
    "    \n",
    "    data.update(data_step4)\n",
    "    data['date'] = pd.to_datetime(data['date'], unit='s')\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    res['date'] = data['date']\n",
    "    res['type'] = data['type']\n",
    "    \n",
    "    res['count_all_asn'] = asn_count[min(asn_count.keys(), key=lambda ts: abs(ts-data['date']))]\n",
    "    res['count_dst_asn_step0'] = len(data['dst_asn_step0'])\n",
    "    res['count_dst_asn_step1'] = len(data['dst_asn_step1'])\n",
    "    res['count_dst_asn_step2'] = len(data['dst_asn_step2'])\n",
    "    res['count_dst_asn_step3'] = len(data['dst_asn_step3'])\n",
    "    res['count_dst_asn_step4'] = len(data['dst_asn_step4'])\n",
    "    \n",
    "    res['ips_all_asn'] = all_ips[min(all_ips.keys(), key=lambda ts: abs(ts-data['date']))]\n",
    "    res['ips_dst_asn_step0'] = get_ips_from_asn_list(data['dst_asn_step0'], data['date'])\n",
    "    res['ips_dst_asn_step1'] = get_ips_from_asn_list(data['dst_asn_step1'], data['date'])\n",
    "    res['ips_dst_asn_step2'] = get_ips_from_asn_list(data['dst_asn_step2'], data['date'])\n",
    "    res['ips_dst_asn_step3'] = get_ips_from_asn_list(data['dst_asn_step3'], data['date'])\n",
    "    res['ips_dst_asn_step4'] = get_ips_from_asn_list(data['dst_asn_step4'], data['date'])\n",
    "        \n",
    "    tmp = calc_bias_pfx_coverage(data)\n",
    "    res['announced_prefixes_24'] = tmp['announced_prefixes_24']\n",
    "    res['announced_prefixes_24_size'] = tmp['announced_prefixes_24_size']\n",
    "    res['covered_pfx_parents_24_step0'] = tmp['covered_pfx_parents_24_step0']\n",
    "    res['covered_pfx_parents_24_size_step0'] = tmp['covered_pfx_parents_24_size_step0']\n",
    "    res['covered_pfx_parents_24_step1'] = tmp['covered_pfx_parents_24_step1']\n",
    "    res['covered_pfx_parents_24_size_step1'] = tmp['covered_pfx_parents_24_size_step1']\n",
    "    res['covered_pfx_parents_24_step2'] = tmp['covered_pfx_parents_24_step2']\n",
    "    res['covered_pfx_parents_24_size_step2'] = tmp['covered_pfx_parents_24_size_step2']\n",
    "    res['covered_pfx_parents_24_step3'] = tmp['covered_pfx_parents_24_step3']\n",
    "    res['covered_pfx_parents_24_size_step3'] = tmp['covered_pfx_parents_24_size_step3']\n",
    "    res['covered_pfx_parents_24_step4'] = tmp['covered_pfx_parents_24_step4']\n",
    "    res['covered_pfx_parents_24_size_step4'] = tmp['covered_pfx_parents_24_size_step4']\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.iglob(ROOT_DIR + 'code/traceroutes/bias_results/*.bz2'))\n",
    "\n",
    "pool = multiprocessing.Pool(12)\n",
    "res_bias = list(tqdm.tqdm_notebook(pool.imap_unordered(process_bias_data, files), total=len(files)))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill step 4\n",
    "for r in res_bias:\n",
    "    keys = r.keys()\n",
    "    for k in keys:\n",
    "        if 'step4' in k:\n",
    "            del(r[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias = pd.DataFrame(res_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias = df_bias.set_index('date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df_bias.pivot(columns='type')\n",
    "exp.columns = exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "exp.to_csv('../csvs/bias.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias['ratio_1'] = df_bias['covered_pfx_parents_24_step0'] / df_bias['announced_prefixes_24']\n",
    "df_bias['ratio_2'] = df_bias['covered_pfx_parents_24_step3'] / df_bias['announced_prefixes_24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias.groupby('type').agg({'ratio_1': 'mean', 'ratio_2': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias['ratio_3'] = df_bias['covered_pfx_parents_24_size_step0'] / df_bias['announced_prefixes_24_size']\n",
    "df_bias['ratio_4'] = df_bias['covered_pfx_parents_24_size_step3'] / df_bias['announced_prefixes_24_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias.groupby('type').agg({'ratio_3': 'mean', 'ratio_4': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'caida-ark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(figsize_a3[1], figsize_a3[0]))\n",
    "\n",
    "df_bias[df_bias['type'] == 'iplane'][['count_all_asn', 'count_dst_asn_step0', 'count_dst_asn_step1', 'count_dst_asn_step2', 'count_dst_asn_step3', 'count_dst_asn_step4']].plot(ax=ax[0][0])\n",
    "df_bias[df_bias['type'] == 'caida-ark'][['count_all_asn', 'count_dst_asn_step0', 'count_dst_asn_step1', 'count_dst_asn_step2', 'count_dst_asn_step3', 'count_dst_asn_step4']].plot(ax=ax[1][0])\n",
    "df_bias[df_bias['type'] == 'iplane'][['ips_all_asn', 'ips_dst_asn_step0', 'ips_dst_asn_step1', 'ips_dst_asn_step2', 'ips_dst_asn_step3', 'ips_dst_asn_step4']].plot(ax=ax[0][1])\n",
    "df_bias[df_bias['type'] == 'caida-ark'][['ips_all_asn', 'ips_dst_asn_step0', 'ips_dst_asn_step1', 'ips_dst_asn_step2', 'ips_dst_asn_step3', 'ips_dst_asn_step4']].plot(ax=ax[1][1])\n",
    "df_bias[df_bias['type'] == 'iplane'][['announced_prefixes_24', 'covered_pfx_parents_24_step0', 'covered_pfx_parents_24_step1', 'covered_pfx_parents_24_step2', 'covered_pfx_parents_24_step3', 'covered_pfx_parents_24_step4']].plot(ax=ax[0][2])\n",
    "df_bias[df_bias['type'] == 'caida-ark'][['announced_prefixes_24', 'covered_pfx_parents_24_step0', 'covered_pfx_parents_24_step1', 'covered_pfx_parents_24_step2', 'covered_pfx_parents_24_step3','covered_pfx_parents_24_step4']].plot(ax=ax[1][2])\n",
    "df_bias[df_bias['type'] == 'iplane'][['announced_prefixes_24_size', 'covered_pfx_parents_24_size_step0', 'covered_pfx_parents_24_size_step1', 'covered_pfx_parents_24_size_step2', 'covered_pfx_parents_24_size_step3', 'covered_pfx_parents_24_size_step4']].plot(ax=ax[0][3])\n",
    "df_bias[df_bias['type'] == 'caida-ark'][['announced_prefixes_24_size', 'covered_pfx_parents_24_size_step0', 'covered_pfx_parents_24_size_step1', 'covered_pfx_parents_24_size_step2', 'covered_pfx_parents_24_size_step3', 'covered_pfx_parents_24_size_step4']].plot(ax=ax[1][3])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/8-figures-for-gianni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'iplane'][['count_all_asn', 'count_dst_asn_step0', 'count_dst_asn_step1', 'count_dst_asn_step2', 'count_dst_asn_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'caida-ark'][['count_all_asn', 'count_dst_asn_step0', 'count_dst_asn_step1', 'count_dst_asn_step2', 'count_dst_asn_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'iplane'][['ips_all_asn', 'ips_dst_asn_step0', 'ips_dst_asn_step1', 'ips_dst_asn_step2', 'ips_dst_asn_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'caida-ark'][['ips_all_asn']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'caida-ark'][['ips_all_asn', 'ips_dst_asn_step0', 'ips_dst_asn_step1', 'ips_dst_asn_step2', 'ips_dst_asn_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'iplane'][['announced_prefixes_24', 'covered_pfx_parents_24_step0', 'covered_pfx_parents_24_step1', 'covered_pfx_parents_24_step2', 'covered_pfx_parents_24_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'caida-ark'][['announced_prefixes_24', 'covered_pfx_parents_24_step0', 'covered_pfx_parents_24_step1', 'covered_pfx_parents_24_step2', 'covered_pfx_parents_24_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'iplane'][['announced_prefixes_24_size', 'covered_pfx_parents_24_size_step0', 'covered_pfx_parents_24_size_step1', 'covered_pfx_parents_24_size_step2', 'covered_pfx_parents_24_size_step3']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[df_bias['type'] == 'caida-ark'][['announced_prefixes_24_size', 'covered_pfx_parents_24_size_step0', 'covered_pfx_parents_24_size_step1', 'covered_pfx_parents_24_size_step2', 'covered_pfx_parents_24_size_step3']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Types v2 - Steve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_data():\n",
    "    for entry in tqdm.tqdm_notebook(db['steve_link_types_v2'].find({'link_counts': {'$ne': 'total'}}, {'_id': 0})):\n",
    "        link_counts = { 'link_%s' % k: v for k, v in dict(entry['link_counts']).iteritems() }\n",
    "        entry.update(link_counts)\n",
    "        del(entry['link_counts'])\n",
    "        yield entry\n",
    "\n",
    "df = pd.DataFrame.from_records(get_data())\n",
    "df = df.fillna(0)\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['peering_route'] = df['link_p'] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average = lambda x: np.average(x.values, weights=df.loc[x.index, 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four-way final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average():\n",
    "    def weighted_average_(x):\n",
    "        return np.average(x.values, weights=df.loc[x.index, 'count'])\n",
    "    weighted_average_.__name__ = 'weighted_average'\n",
    "    return weighted_average_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_percentile(n):        \n",
    "    def reconstruced_percentile_(x):\n",
    "        n_counts = df.loc[x.index, 'count'].values\n",
    "        x = x.values\n",
    "        counts_reconstruct = defaultdict(lambda: 0)\n",
    "\n",
    "        for hops, count in zip(x, n_counts):\n",
    "            counts_reconstruct[hops] += count\n",
    "            \n",
    "        return np.percentile(list(collections.Counter(counts_reconstruct).elements()), n)\n",
    "    \n",
    "    reconstruced_percentile_.__name__ = 'percentile_%s' % n\n",
    "    return reconstruced_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gb = df.groupby(['date', 'type', 'ixp_hops', 'peering_route']).agg({'as_hops': [reconstructed_percentile(10), reconstructed_percentile(90), weighted_average()], 'ip_hops': [reconstructed_percentile(10), reconstructed_percentile(90), weighted_average()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = gb.unstack().unstack().unstack()\n",
    "exp.columns = exp.columns.reorder_levels([4, 0, 2, 3, 1])\n",
    "exp = exp.reindex(exp.columns.sort_values(), axis='columns')\n",
    "exp.columns = ['/'.join(map(str,c)) for c in exp.columns]\n",
    "display(exp.head(25))\n",
    "exp.to_csv('../csvs/four-way-split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=figsize_a4)\n",
    "\n",
    "gb = df.groupby(['date', 'type', 'ixp_hops', 'peering_route']).agg({'as_hops': weighted_average, 'ip_hops': weighted_average, 'count': 'sum'})\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][0])\n",
    "ax[0][0].set_title('AS hops - iPlane')\n",
    "ax[0][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][1])\n",
    "ax[0][1].set_title('AS hops - Ark')\n",
    "ax[0][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][0])\n",
    "ax[1][0].set_title('IP hops - iPlane')\n",
    "ax[1][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][1])\n",
    "ax[1][1].set_title('IP hops - Ark')\n",
    "ax[1][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][0])\n",
    "ax[2][0].set_title('Count - iPlane')\n",
    "ax[2][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][1])\n",
    "ax[2][1].set_title('Count - Ark')\n",
    "ax[2][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/traces-four-way-split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=figsize_a4)\n",
    "\n",
    "gb = df.groupby(['date', 'type', 'ixp_hops', 'peering_route']).agg({'as_hops': reconstructed_median, 'ip_hops': reconstructed_median, 'count': 'sum'})\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][0])\n",
    "ax[0][0].set_title('AS hops - iPlane')\n",
    "ax[0][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][1])\n",
    "ax[0][1].set_title('AS hops - Ark')\n",
    "ax[0][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][0])\n",
    "ax[1][0].set_title('IP hops - iPlane')\n",
    "ax[1][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][1])\n",
    "ax[1][1].set_title('IP hops - Ark')\n",
    "ax[1][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][0])\n",
    "ax[2][0].set_title('Count - iPlane')\n",
    "ax[2][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][1])\n",
    "ax[2][1].set_title('Count - Ark')\n",
    "ax[2][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/traces-four-way-split-median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_median(x):\n",
    "    n_counts = df.loc[x.index, 'count'].values\n",
    "    x = x.values\n",
    "    counts_reconstruct = defaultdict(lambda: 0)\n",
    "    \n",
    "    for hops, count in zip(x, n_counts):\n",
    "        counts_reconstruct[hops] += count\n",
    "        \n",
    "    return(np.median(list(collections.Counter(counts_reconstruct).elements())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_percentile(n):        \n",
    "    def reconstruced_percentile_(x):\n",
    "        n_counts = df.loc[x.index, 'count'].values\n",
    "        x = x.values\n",
    "        counts_reconstruct = defaultdict(lambda: 0)\n",
    "\n",
    "        for hops, count in zip(x, n_counts):\n",
    "            counts_reconstruct[hops] += count\n",
    "            \n",
    "        return np.percentile(list(collections.Counter(counts_reconstruct).elements()), n)\n",
    "    \n",
    "    reconstruced_percentile_.__name__ = 'percentile_%s' % n\n",
    "    return reconstruced_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'type', 'ixp_hops', 'peering_route']).agg({'as_hops': [reconstructed_percentile(10), reconstructed_percentile(90), reconstructed_median], 'ip_hops': [reconstructed_percentile(10), reconstructed_percentile(90), reconstructed_median]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 4, figsize=(2*figsize_a3[0], figsize_a3[1]))\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', 'reconstructed_median', u'iplane'), ('as_hops', 'percentile_10', u'iplane'), ('as_hops', 'percentile_90', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'iplane', False, 0.0), ('percentile_10', u'iplane', False, 0.0), ('percentile_90', u'iplane', False, 0.0)]].plot(ax=ax[0][0])\n",
    "p[[('reconstructed_median', u'iplane', False, 1.0), ('percentile_10', u'iplane', False, 1.0), ('percentile_90', u'iplane', False, 1.0)]].plot(ax=ax[0][1])\n",
    "p[[('reconstructed_median', u'iplane', True, 0.0), ('percentile_10', u'iplane', True, 0.0), ('percentile_90', u'iplane', True, 0.0)]].plot(ax=ax[0][2])\n",
    "p[[('reconstructed_median', u'iplane', True, 1.0), ('percentile_10', u'iplane', True, 1.0), ('percentile_90', u'iplane', True, 1.0)]].plot(ax=ax[0][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[0][i].set_title('AS hops - iPlane')\n",
    "    ax[0][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', 'reconstructed_median', u'caida-ark'), ('as_hops', 'percentile_10', u'caida-ark'), ('as_hops', 'percentile_90', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'caida-ark', False, 0.0), ('percentile_10', u'caida-ark', False, 0.0), ('percentile_90', u'caida-ark', False, 0.0)]].plot(ax=ax[1][0])\n",
    "p[[('reconstructed_median', u'caida-ark', False, 1.0), ('percentile_10', u'caida-ark', False, 1.0), ('percentile_90', u'caida-ark', False, 1.0)]].plot(ax=ax[1][1])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 0.0), ('percentile_10', u'caida-ark', True, 0.0), ('percentile_90', u'caida-ark', True, 0.0)]].plot(ax=ax[1][2])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 1.0), ('percentile_10', u'caida-ark', True, 1.0), ('percentile_90', u'caida-ark', True, 1.0)]].plot(ax=ax[1][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[1][i].set_title('AS hops - Ark')\n",
    "    ax[1][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', 'reconstructed_median', u'iplane'), ('ip_hops', 'percentile_10', u'iplane'), ('ip_hops', 'percentile_90', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'iplane', False, 0.0), ('percentile_10', u'iplane', False, 0.0), ('percentile_90', u'iplane', False, 0.0)]].plot(ax=ax[2][0])\n",
    "p[[('reconstructed_median', u'iplane', False, 1.0), ('percentile_10', u'iplane', False, 1.0), ('percentile_90', u'iplane', False, 1.0)]].plot(ax=ax[2][1])\n",
    "p[[('reconstructed_median', u'iplane', True, 0.0), ('percentile_10', u'iplane', True, 0.0), ('percentile_90', u'iplane', True, 0.0)]].plot(ax=ax[2][2])\n",
    "p[[('reconstructed_median', u'iplane', True, 1.0), ('percentile_10', u'iplane', True, 1.0), ('percentile_90', u'iplane', True, 1.0)]].plot(ax=ax[2][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[2][i].set_title('IP hops - iPlane')\n",
    "    ax[2][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', 'reconstructed_median', u'caida-ark'), ('ip_hops', 'percentile_10', u'caida-ark'), ('ip_hops', 'percentile_90', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'caida-ark', False, 0.0), ('percentile_10', u'caida-ark', False, 0.0), ('percentile_90', u'caida-ark', False, 0.0)]].plot(ax=ax[3][0])\n",
    "p[[('reconstructed_median', u'caida-ark', False, 1.0), ('percentile_10', u'caida-ark', False, 1.0), ('percentile_90', u'caida-ark', False, 1.0)]].plot(ax=ax[3][1])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 0.0), ('percentile_10', u'caida-ark', True, 0.0), ('percentile_90', u'caida-ark', True, 0.0)]].plot(ax=ax[3][2])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 1.0), ('percentile_10', u'caida-ark', True, 1.0), ('percentile_90', u'caida-ark', True, 1.0)]].plot(ax=ax[3][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[3][i].set_title('IP hops - Ark')\n",
    "    ax[3][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/traces-four-way-split-median-q10-q90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Types v2 - Steve - Hypergiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_data():\n",
    "    for entry in tqdm.tqdm_notebook(db['steve_link_types_v2_hypergiants'].find({'link_counts': {'$ne': 'total'}}, {'_id': 0})):\n",
    "        link_counts = { 'link_%s' % k: v for k, v in dict(entry['link_counts']).iteritems() }\n",
    "        entry.update(link_counts)\n",
    "        del(entry['link_counts'])\n",
    "        yield entry\n",
    "\n",
    "df = pd.DataFrame.from_records(get_data())\n",
    "df = df.fillna(0)\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['peering_route'] = df['link_p'] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average = lambda x: np.average(x.values, weights=df.loc[x.index, 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=figsize_a4)\n",
    "\n",
    "gb = df.groupby(['date', 'type', 'ixp_hops', 'peering_route']).agg({'as_hops': weighted_average, 'ip_hops': weighted_average, 'count': 'sum'})\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][0])\n",
    "ax[0][0].set_title('AS hops - iPlane')\n",
    "ax[0][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][1])\n",
    "ax[0][1].set_title('AS hops - Ark')\n",
    "ax[0][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][0])\n",
    "ax[1][0].set_title('IP hops - iPlane')\n",
    "ax[1][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][1])\n",
    "ax[1][1].set_title('IP hops - Ark')\n",
    "ax[1][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][0])\n",
    "ax[2][0].set_title('Count - iPlane')\n",
    "ax[2][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][1])\n",
    "ax[2][1].set_title('Count - Ark')\n",
    "ax[2][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/traces-four-way-split-hypergiants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_percentile(n):        \n",
    "    def reconstruced_percentile_(x):\n",
    "        n_counts = df.loc[x.index, 'count'].values\n",
    "        x = x.values\n",
    "        counts_reconstruct = defaultdict(lambda: 0)\n",
    "\n",
    "        for hops, count in zip(x, n_counts):\n",
    "            counts_reconstruct[hops] += count\n",
    "            \n",
    "        return np.percentile(list(collections.Counter(counts_reconstruct).elements()), n)\n",
    "    \n",
    "    reconstruced_percentile_.__name__ = 'percentile_%s' % n\n",
    "    return reconstruced_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['date', 'type', 'ixp_hops', 'peering_route']).agg({'as_hops': [reconstructed_percentile(10), reconstructed_percentile(90), reconstructed_median], 'ip_hops': [reconstructed_percentile(10), reconstructed_percentile(90), reconstructed_median]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 4, figsize=(2*figsize_a3[0], figsize_a3[1]))\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', 'reconstructed_median', u'iplane'), ('as_hops', 'percentile_10', u'iplane'), ('as_hops', 'percentile_90', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'iplane', False, 0.0), ('percentile_10', u'iplane', False, 0.0), ('percentile_90', u'iplane', False, 0.0)]].plot(ax=ax[0][0])\n",
    "p[[('reconstructed_median', u'iplane', False, 1.0), ('percentile_10', u'iplane', False, 1.0), ('percentile_90', u'iplane', False, 1.0)]].plot(ax=ax[0][1])\n",
    "p[[('reconstructed_median', u'iplane', True, 0.0), ('percentile_10', u'iplane', True, 0.0), ('percentile_90', u'iplane', True, 0.0)]].plot(ax=ax[0][2])\n",
    "p[[('reconstructed_median', u'iplane', True, 1.0), ('percentile_10', u'iplane', True, 1.0), ('percentile_90', u'iplane', True, 1.0)]].plot(ax=ax[0][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[0][i].set_title('AS hops - iPlane')\n",
    "    ax[0][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', 'reconstructed_median', u'caida-ark'), ('as_hops', 'percentile_10', u'caida-ark'), ('as_hops', 'percentile_90', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'caida-ark', False, 0.0), ('percentile_10', u'caida-ark', False, 0.0), ('percentile_90', u'caida-ark', False, 0.0)]].plot(ax=ax[1][0])\n",
    "p[[('reconstructed_median', u'caida-ark', False, 1.0), ('percentile_10', u'caida-ark', False, 1.0), ('percentile_90', u'caida-ark', False, 1.0)]].plot(ax=ax[1][1])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 0.0), ('percentile_10', u'caida-ark', True, 0.0), ('percentile_90', u'caida-ark', True, 0.0)]].plot(ax=ax[1][2])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 1.0), ('percentile_10', u'caida-ark', True, 1.0), ('percentile_90', u'caida-ark', True, 1.0)]].plot(ax=ax[1][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[1][i].set_title('AS hops - Ark')\n",
    "    ax[1][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', 'reconstructed_median', u'iplane'), ('ip_hops', 'percentile_10', u'iplane'), ('ip_hops', 'percentile_90', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'iplane', False, 0.0), ('percentile_10', u'iplane', False, 0.0), ('percentile_90', u'iplane', False, 0.0)]].plot(ax=ax[2][0])\n",
    "p[[('reconstructed_median', u'iplane', False, 1.0), ('percentile_10', u'iplane', False, 1.0), ('percentile_90', u'iplane', False, 1.0)]].plot(ax=ax[2][1])\n",
    "p[[('reconstructed_median', u'iplane', True, 0.0), ('percentile_10', u'iplane', True, 0.0), ('percentile_90', u'iplane', True, 0.0)]].plot(ax=ax[2][2])\n",
    "p[[('reconstructed_median', u'iplane', True, 1.0), ('percentile_10', u'iplane', True, 1.0), ('percentile_90', u'iplane', True, 1.0)]].plot(ax=ax[2][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[2][i].set_title('IP hops - iPlane')\n",
    "    ax[2][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', 'reconstructed_median', u'caida-ark'), ('ip_hops', 'percentile_10', u'caida-ark'), ('ip_hops', 'percentile_90', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel()\n",
    "p[[('reconstructed_median', u'caida-ark', False, 0.0), ('percentile_10', u'caida-ark', False, 0.0), ('percentile_90', u'caida-ark', False, 0.0)]].plot(ax=ax[3][0])\n",
    "p[[('reconstructed_median', u'caida-ark', False, 1.0), ('percentile_10', u'caida-ark', False, 1.0), ('percentile_90', u'caida-ark', False, 1.0)]].plot(ax=ax[3][1])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 0.0), ('percentile_10', u'caida-ark', True, 0.0), ('percentile_90', u'caida-ark', True, 0.0)]].plot(ax=ax[3][2])\n",
    "p[[('reconstructed_median', u'caida-ark', True, 1.0), ('percentile_10', u'caida-ark', True, 1.0), ('percentile_90', u'caida-ark', True, 1.0)]].plot(ax=ax[3][3])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[3][i].set_title('IP hops - Ark')\n",
    "    ax[3][i].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/traces-four-way-split-hypergiants-median-q10-q90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=figsize_a4)\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][0])\n",
    "ax[0][0].set_title('AS hops - iPlane')\n",
    "ax[0][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('as_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[0][1])\n",
    "ax[0][1].set_title('AS hops - Ark')\n",
    "ax[0][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][0])\n",
    "ax[1][0].set_title('IP hops - iPlane')\n",
    "ax[1][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('ip_hops', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[1][1])\n",
    "ax[1][1].set_title('IP hops - Ark')\n",
    "ax[1][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'iplane')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][0])\n",
    "ax[2][0].set_title('Count - iPlane')\n",
    "ax[2][0].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "p = gb.unstack('type')[[('count', u'caida-ark')]].unstack().unstack()\n",
    "p.columns = p.columns.droplevel().droplevel()\n",
    "p.plot(ax=ax[2][1])\n",
    "ax[2][1].set_title('Count - Ark')\n",
    "ax[2][1].legend(frameon=True, title='peering_route, ixp_hops')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figures/traces-four-way-split-hypergiants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Types v2 - Steve - Hypergiants + fourway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_data():\n",
    "    collection = db['steve_link_types_v2_hypergiants_plus_subnet']\n",
    "    for entry in tqdm.tqdm_notebook(collection.find({'link_counts': {'$ne': 'total'}}, {'_id': 0}), total=collection.count()):\n",
    "        link_counts = { 'link_%s' % k: v for k, v in dict(entry['link_counts']).iteritems() }\n",
    "        entry.update(link_counts)\n",
    "        del(entry['link_counts'])\n",
    "        yield entry\n",
    "\n",
    "df = pd.DataFrame.from_records(get_data())\n",
    "df = df.fillna(0)\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subnet24'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
